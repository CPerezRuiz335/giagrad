# giagrad
Deep learning framework made by and for students.

Like https://m.youtube.com/watch?v=VMj-3S1tku0&t=7903s
but more like tinygrad but with the spirit of numpy_ml but more PyTorchish.
See https://m.youtube.com/watch?v=VMj-3S1tku0&t=7903s to understand
# TODO

- test reciprocal and log mathops
- start Wiki
    * home, philosofy, why? (LEARNING(code, git, IA), for CV (pragmatic), learning by doing, community based, etc)
    * Guidelines (code style, PEP8, language (english code, comments README, wiki in spanish/catalan), etc)
        - structure tests
        - structure optimizers
    * Contributing (explain branches, forks, workflow, issues, discussions, etc)
    * TODOs in wiki and upload if necessary    
- REZAR PARA QUE FUNCIONE


# MILESTONES
- finish operators, even activation functions
- first dense Layer (wx + b)
- make demo of MNIST training, sell project to newbies in README (regression ugly)
- make data directory for small projects/tests (optional)
- first Optimizer (SGD, Adam?), make cool visualization for feedback
    * make cool gif for optimizers comparisons for everyone implementing one (class pattern etc)
      and put in Wiki or README, just one executable that GLOBS every file inside optimizers
      and creates gif. You can make your own optimizer :man_shrugging:

# FUTURE
- Conv layer
- Natural Language implementations


# PROBLEMS
- optimization and speed VS simplicity and self-explained code for newbies
- lack of community
- not knowing how to manage the fuck I have in mind


# SIDE OBJECTIVE
- code almost everything popular in AI even transformers

# FUTURE BRANCHES

- nn/layers
- optimizers
- loss functions
- documentation
- visualization (meh, could be nice, I suppose someone has to do it)

