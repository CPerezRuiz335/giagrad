# giagrad
Deep learning framework made by and for students.

Like [micrograd](https://youtu.be/VMj-3S1tku0). More like [tinygrad](https://github.com/geohot/tinygrad) but with the spirit of 
[numpy_ml](https://numpy-ml.readthedocs.io/en/latest/) but more PyTorch-ish. See [micrograd](https://youtu.be/VMj-3S1tku0) to understand.

# TODO
- Test:
    * reduction operators
    * layers
    * softmax, logSoftmax
    * SGD Optimizer
    * CrossEntropyLoss
    * gradient update in batches vs stochastic

# PROBLEMS
- optimization and speed VS simplicity and self-explained code for newbies
- lack of contributions/community :man_shrugging:

# GOAL
- code almost everything popular in AI even transformers

# FUTURE BRANCHES/IDEAS
- nn/layers
- optimizers
- loss functions
- documentation
- visualization (meh, could be nice, I suppose someone has to do it)
