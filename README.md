# giagrad
Deep learning framework made by and for students.

Like [micrograd]{https://youtu.be/VMj-3S1tku0}. More like tinygrad but with the spirit of 
numpy_ml but more PyTorch-ish. See [micrograd]{https://youtu.be/VMj-3S1tku0} to understand.

# TODO
- Test:
    * reduction operators
    * layers
    * softmax, logSoftmax
    * SGD Optimizer
    * CrossEntropyLoss
    * gradient update in batches vs stochastic

# PROBLEMS
- optimization and speed VS simplicity and self-explained code for newbies
- lack of community

# GOAL
- code almost everything popular in AI even transformers

# FUTURE BRANCHES/IDEAS
- nn/layers
- optimizers
- loss functions
- documentation
- visualization (meh, could be nice, I suppose someone has to do it)