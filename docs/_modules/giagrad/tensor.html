

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>giagrad.tensor &#8212; giagrad  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx_paramlinks.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/giagrad/tensor';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">giagrad  documentation</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../autograd.html">
                        Autograd
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../giagrad.html">
                        giagrad
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../nn.html">
                        giagrad.nn
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../nn_activations.html">
                        giagrad.nn.activations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../optim.html">
                        giagrad.optim
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../display.html">
                        giagrad.display
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/CPerezRuiz335/giagrad" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../autograd.html">
                        Autograd
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../giagrad.html">
                        giagrad
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../nn.html">
                        giagrad.nn
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../nn_activations.html">
                        giagrad.nn.activations
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../optim.html">
                        giagrad.optim
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../display.html">
                        giagrad.display
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/CPerezRuiz335/giagrad" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">giagrad.tensor</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for giagrad.tensor</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">numpy.typing</span> <span class="kn">import</span> <span class="n">NDArray</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>

<span class="k">class</span> <span class="nc">Function</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="s1">&#39;parents&#39;</span><span class="p">,</span> <span class="s1">&#39;_name&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">def</span> <span class="nf">save_for_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves parent tensors in :attr:`parents` for backward pass.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *tensors: Tensor, ...</span>
<span class="sd">            The input tensors of :meth:`~giagrad.Function.forward`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">),</span> \
        <span class="s2">&quot;parents must not contain other types than Tensor&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span>

<div class="viewcode-block" id="Function.forward"><a class="viewcode-back" href="../../generated/giagrad.tensor.Function.forward.html#giagrad.Function.forward">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Makes forward pass.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *tensors: Tensor</span>
<span class="sd">            A variable number of tensors, e.g. two for binary operations</span>
<span class="sd">            such as :func:`~giagrad.Tensor.matmul`.</span>

<span class="sd">        **kwargs: </span>
<span class="sd">            Optional arguments if needed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ndarray or float:</span>
<span class="sd">            The result of applying that operation to :paramref:`*tensors`&#39;s </span>
<span class="sd">            data, i.g. float for reduction operations in some cases or a</span>
<span class="sd">            new ndarray.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;forward not implemented for </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Function.backward"><a class="viewcode-back" href="../../generated/giagrad.tensor.Function.backward.html#giagrad.Function.backward">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partial</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Backpropagate from child tensor created with :func:`~giagrad.Tensor.comm`.</span>
<span class="sd">        </span>
<span class="sd">        Updates :attr:`~parents` gradient through chain rule. This </span>
<span class="sd">        method is the extension of :func:`~giagrad.Tensor.backward` for </span>
<span class="sd">        a concrete operation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        partial: ndarray</span>
<span class="sd">            Defines the partial derivative of the loss function with </span>
<span class="sd">            respect to the child Tensor, the one created with </span>
<span class="sd">            :func:`~giagrad.tensor.Function.forward`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;backward not implemented for </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> 

<span class="kn">import</span> <span class="nn">giagrad.shapeops</span> <span class="k">as</span> <span class="nn">sops</span>
<span class="kn">import</span> <span class="nn">giagrad.mathops</span> <span class="k">as</span> <span class="nn">mops</span>
<span class="kn">import</span> <span class="nn">giagrad.reductionops</span> <span class="k">as</span> <span class="nn">rops</span>
<span class="kn">import</span> <span class="nn">giagrad.mlops</span> <span class="k">as</span> <span class="nn">mlops</span>
<span class="kn">import</span> <span class="nn">giagrad.initializers</span> <span class="k">as</span> <span class="nn">init</span>

<span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="n">__array_ufunc__</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tell numpy to trust Tensor to make __r***__ method</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;fn&#39;</span><span class="p">,</span> <span class="s1">&#39;requires_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> 
        <span class="n">data</span><span class="p">,</span> 
        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
        <span class="n">fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Function</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="k">else</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="k">if</span> <span class="n">requires_grad</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    
    <span class="c1"># ***** backprop *****</span>
<div class="viewcode-block" id="Tensor.backward"><a class="viewcode-back" href="../../generated/giagrad.Tensor.backward.html#giagrad.Tensor.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the gradient of all preceeding tensors.</span>
<span class="sd">        </span>
<span class="sd">        The graph is differentiated using the chain rule. Whether it is</span>
<span class="sd">        scalar or non-scalar (i.e. its data has more than one element), </span>
<span class="sd">        gradient is set to ones and backpropagated.</span>

<span class="sd">        This function accumulates gradients in every preceeding tensor, </span>
<span class="sd">        you might need to zero .grad attributes or set them to None </span>
<span class="sd">        before calling it. </span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        retain_graph: bool, default: False</span>
<span class="sd">            If ``False`` the graph used to compute the grads will be freed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="bp">self</span><span class="p">])</span>    
        <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">function</span> <span class="o">:=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">fn</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">function</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                        <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                        <span class="n">build_topo</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="n">topo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="n">build_topo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># chain rule </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="c1"># dL/dL = 1</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">fn</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">retain_graph</span><span class="p">:</span> 
                <span class="n">tensor</span><span class="o">.</span><span class="n">fn</span> <span class="o">=</span> <span class="kc">None</span> 

        <span class="k">del</span> <span class="n">topo</span><span class="p">,</span> <span class="n">visited</span> <span class="c1"># outsmart gargabe collector</span></div>

    <span class="c1"># ***** helpers *****</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tuple of tensor dimensions.</span>

<span class="sd">        Unlike numpy.ndarray.shape it can not be used to </span>
<span class="sd">        reshape inplace.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">type</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Data-type of the tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Size of the tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Number of dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ndim</span>

<div class="viewcode-block" id="Tensor.no_grad"><a class="viewcode-back" href="../../generated/giagrad.Tensor.no_grad.html#giagrad.Tensor.no_grad">[docs]</a>    <span class="k">def</span> <span class="nf">no_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Makes tensor not autodifferentiable.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.requires_grad_"><a class="viewcode-back" href="../../generated/giagrad.Tensor.requires_grad_.html#giagrad.Tensor.requires_grad_">[docs]</a>    <span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Makes tensor autodifferentiable.&quot;&quot;&quot;</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="s1">&#39;tensor: &#39;</span> 
            <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array2string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;tensor: &#39;</span><span class="p">)</span> 
            <span class="o">+</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot; fn: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fn</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fn</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> 
            <span class="o">+</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;, name: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># ***** initializers in-place*****</span>
<div class="viewcode-block" id="Tensor.empty"><a class="viewcode-back" href="../../generated/giagrad.Tensor.empty.html#giagrad.Tensor.empty">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a tensor filled with uninitialized data. </span>

<span class="sd">        Datatype is ``np.float32`` by default.</span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        shape: int, ...</span>
<span class="sd">            A variable number of integers defining the shape of the </span>
<span class="sd">            output tensor.</span>
<span class="sd">        \*\*kwargs:</span>
<span class="sd">            Parameters passed to the Tensor class initializer.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3, requires_grad=True, dtype=np.float64)</span>
<span class="sd">        tensor: [[4.67662529e-310 0.00000000e+000 4.67596337e-310]</span>
<span class="sd">                 [6.94592882e-310 6.94611561e-310 6.94609055e-310]]    </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="c1"># in-place initializers</span>
<div class="viewcode-block" id="Tensor.zeros"><a class="viewcode-back" href="../../generated/giagrad.Tensor.zeros.html#giagrad.Tensor.zeros">[docs]</a>    <span class="k">def</span> <span class="nf">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with zeros. </span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        -------_</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3).zeros()                                                                                           </span>
<span class="sd">        tensor: [[0. 0. 0.]</span>
<span class="sd">                 [0. 0. 0.]]  </span>
<span class="sd">        &gt;&gt;&gt; Tensor([1, 3, 4, 5]).zeros()</span>
<span class="sd">        tensor: [0., 0., 0., 0.] </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.ones"><a class="viewcode-back" href="../../generated/giagrad.Tensor.ones.html#giagrad.Tensor.ones">[docs]</a>    <span class="k">def</span> <span class="nf">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with ones. </span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3).ones()                                                                                           </span>
<span class="sd">        tensor: [[1. 1. 1.]</span>
<span class="sd">                 [1. 1. 1.]]  </span>
<span class="sd">        &gt;&gt;&gt; Tensor([1, 3, 4, 5]).ones()</span>
<span class="sd">        tensor: [1., 1., 1., 1.] </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.constant"><a class="viewcode-back" href="../../generated/giagrad.Tensor.constant.html#giagrad.Tensor.constant">[docs]</a>    <span class="k">def</span> <span class="nf">constant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with a constant value. </span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        fill_value: float</span>
<span class="sd">            The value to fill the tensor with.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3).constant(2.71828)                                                                                           </span>
<span class="sd">        tensor: [[2.71828 2.71828 2.71828]</span>
<span class="sd">                 [2.71828 2.71828 2.71828]]  </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="n">fill_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
        
<div class="viewcode-block" id="Tensor.normal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.normal.html#giagrad.Tensor.normal">[docs]</a>    <span class="k">def</span> <span class="nf">normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with values drawn from the normal</span>
<span class="sd">        distribution :math:`\mathcal{N}(\text{mu}, \text{std}^2)`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        mu: float</span>
<span class="sd">            Mean of the normal distribution.</span>
<span class="sd">        std: float</span>
<span class="sd">            The standard deviation of the normal distribution.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 3).normal()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.uniform"><a class="viewcode-back" href="../../generated/giagrad.Tensor.uniform.html#giagrad.Tensor.uniform">[docs]</a>    <span class="k">def</span> <span class="nf">uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with values drawn from the uniform</span>
<span class="sd">        distribution :math:`\mathcal{U}(a, b)`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a: float</span>
<span class="sd">            The lower bound of the uniform distribution.</span>
<span class="sd">        b: float</span>
<span class="sd">            The upper bound of the uniform distribution.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 3).uniform()</span>
<span class="sd">        &quot;&quot;&quot;</span> 
        <span class="n">init</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.dirac"><a class="viewcode-back" href="../../generated/giagrad.Tensor.dirac.html#giagrad.Tensor.dirac">[docs]</a>    <span class="k">def</span> <span class="nf">dirac</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills the {3, 4, 5}-dimensional Tensor data with the Dirac </span>
<span class="sd">        delta function. </span>

<span class="sd">        Preserves the identity of the inputs in *Convolutional* layers, </span>
<span class="sd">        where as many input channels are preserved as possible. In case</span>
<span class="sd">        of groups &gt; 1, each group of channels preserves identity.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        groups: int, default: 1</span>
<span class="sd">            Number of groups in the conv layer.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 24, 5, 5).dirac(3)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">dirac</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.xavier_uniform"><a class="viewcode-back" href="../../generated/giagrad.Tensor.xavier_uniform.html#giagrad.Tensor.xavier_uniform">[docs]</a>    <span class="k">def</span> <span class="nf">xavier_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known Glorot uniform initialization.</span>

<span class="sd">        This methos is described in `Understanding the difficulty of </span>
<span class="sd">        training deep feedforward neural networks` - Glorot, X. &amp; Bengio, </span>
<span class="sd">        Y. (2010), using a uniform distribution. Tensor data will have </span>
<span class="sd">        values sampled from :math:`\mathcal{U}(-a, a)` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            a = \text{gain} \times \sqrt{\frac{6}{\text{fan_in} + \text{fan_out}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gain: float</span>
<span class="sd">            An optional scaling factor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from giagrad import calculate_gain</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).xavier_uniform(gain=calculate_gain(&#39;relu&#39;))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
    
<div class="viewcode-block" id="Tensor.xavier_normal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.xavier_normal.html#giagrad.Tensor.xavier_normal">[docs]</a>    <span class="k">def</span> <span class="nf">xavier_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known Glorot normal initialization.</span>

<span class="sd">        This method is described in `Understanding the difficulty of </span>
<span class="sd">        training deep feedforward neural networks` - Glorot, X. &amp; Bengio,</span>
<span class="sd">        Y. (2010), using a normal distribution. Tensor data will have </span>
<span class="sd">        values sampled from :math:`\mathcal{N}(0, \sigma^2)` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \sigma = \text{gain} \times \sqrt{\frac{2}{\text{fan_in} + \text{fan_out}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gain: float</span>
<span class="sd">            An optional scaling factor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from giagrad import calculate_gain</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).xavier_normal(gain=calculate_gain(&#39;relu&#39;))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>    </div>

<div class="viewcode-block" id="Tensor.kaiming_uniform"><a class="viewcode-back" href="../../generated/giagrad.Tensor.kaiming_uniform.html#giagrad.Tensor.kaiming_uniform">[docs]</a>    <span class="k">def</span> <span class="nf">kaiming_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known He uniform initialization.</span>

<span class="sd">        Tensor data is filled with values according to the method </span>
<span class="sd">        described in `Delving deep into rectifiers`_ using uniform </span>
<span class="sd">        distribution. The resulting tensor will have values sampled </span>
<span class="sd">        from :math:`\mathcal{U}(-\text{bound}, \text{bound})` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan_mode}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        neg_slope: float</span>
<span class="sd">            The negative slope of the rectifier used after this layer </span>
<span class="sd">            (only used with `&#39;leaky_relu&#39;`).</span>
<span class="sd">        mode: str, default: &#39;fan_in&#39;</span>
<span class="sd">            Either `&#39;fan_in&#39;` or `&#39;fan_out&#39;`. Choosing `&#39;fan_in&#39;`</span>
<span class="sd">            preserves the magnitude of the variance of the weights in </span>
<span class="sd">            the forward pass. Choosing `&#39;fan_out&#39;` preserves the </span>
<span class="sd">            magnitudes in the backwards pass.</span>
<span class="sd">        nonlinearity: str, default: &#39;leaky_relu&#39;</span>
<span class="sd">            The non-linear function method name, recommended to use only </span>
<span class="sd">            with `&#39;relu&#39;` or `&#39;leaky_relu&#39;`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).kaiming_uniform(mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)</span>

<span class="sd">        .. _Delving deep into rectifiers: https://arxiv.org/abs/1502.01852</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.kaiming_normal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.kaiming_normal.html#giagrad.Tensor.kaiming_normal">[docs]</a>    <span class="k">def</span> <span class="nf">kaiming_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known He normal initialization.</span>

<span class="sd">        Tensor data is filled with values according to the method </span>
<span class="sd">        described in `Delving deep into rectifiers`_ using normal </span>
<span class="sd">        distribution. The resulting tensor will have values sampled </span>
<span class="sd">        from :math:`\mathcal{N}(0, \sigma^2)` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \sigma = \frac{\text{gain}}{\sqrt{\text{fan_mode}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        neg_slope: float</span>
<span class="sd">            The negative slope of the rectifier used after this layer </span>
<span class="sd">            (only used with `&#39;leaky_relu&#39;`).</span>
<span class="sd">        mode: str, default: &#39;fan_in&#39;</span>
<span class="sd">            Either `&#39;fan_in&#39;` or `&#39;fan_out&#39;`. Choosing `&#39;fan_in&#39;`</span>
<span class="sd">            preserves the magnitude of the variance of the weights in </span>
<span class="sd">            the forward pass. Choosing `&#39;fan_out&#39;` preserves the </span>
<span class="sd">            magnitudes in the backwards pass.</span>
<span class="sd">        nonlinearity: str, default: &#39;leaky_relu&#39;</span>
<span class="sd">            The non-linear function method name,</span>
<span class="sd">            recommended to use only with `&#39;relu&#39;` or `&#39;leaky_relu&#39;`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).kaiming_normal(mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)</span>

<span class="sd">        .. _Delving deep into rectifiers: https://arxiv.org/abs/1502.01852</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>    </div>

<div class="viewcode-block" id="Tensor.sparse"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sparse.html#giagrad.Tensor.sparse">[docs]</a>    <span class="k">def</span> <span class="nf">sparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills the 2D Tensor data as a sparse matrix.</span>

<span class="sd">        Non-zero elements will be drawn from the normal distribution</span>
<span class="sd">        :math:`\mathcal{N}(0, \text{sigma})`, as described in `Deep </span>
<span class="sd">        learning via Hessian-free optimization`_ - Martens, J. (2010).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        sparsity: float between [0, 1) </span>
<span class="sd">            The fraction of elements in each column to be set to zero.</span>
<span class="sd">        std: float </span>
<span class="sd">            The standard deviation of the normal distribution used to </span>
<span class="sd">            generate the non-zero values.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).sparse(sparsity=0.4, std=0.2)</span>

<span class="sd">        .. _Deep learning via Hessian-free optimization: https://dl.acm.org/doi/10.5555/3104322.3104416</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.orthogonal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.orthogonal.html#giagrad.Tensor.orthogonal">[docs]</a>    <span class="k">def</span> <span class="nf">orthogonal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with a (semi) orthogonal matrix.</span>

<span class="sd">        Values are generated according to the method described in Exact </span>
<span class="sd">        solutions to the nonlinear dynamics of learning in deep linear </span>
<span class="sd">        neural networks - Saxe, A. et al. (2013). The Tensor must have</span>
<span class="sd">        at least 2 dimensions, and for Tensors with more than 2 </span>
<span class="sd">        dimensions the trailing dimensions are flattened.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gain: float</span>
<span class="sd">            Optional scaling factor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).orthogonal()</span>
<span class="sd">        &quot;&quot;&quot;</span> 
        <span class="n">init</span><span class="o">.</span><span class="n">orthogonal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="c1">### MATH ###</span>
<div class="viewcode-block" id="Tensor.comm"><a class="viewcode-back" href="../../generated/giagrad.Tensor.comm.html#giagrad.Tensor.comm">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">comm</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">function</span><span class="p">:</span> <span class="n">Function</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new instance of an autodifferentiable tensor given a </span>
<span class="sd">        :class:`giagrad.tensor.Function`.</span>

<span class="sd">        ``comm`` creates a tensor with the output of </span>
<span class="sd">        :func:`~giagrad.tensor.Function.forward`.</span>
<span class="sd">    </span>
<span class="sd">        For developer use.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *tensors: array_like, ... </span>
<span class="sd">            Internally ``comm`` transforms any object in ``*tensors`` to </span>
<span class="sd">            a Tensor and passes it to :func:`~giagrad.tensor.Function.forward`.</span>

<span class="sd">        *kwargs: </span>
<span class="sd">            Optional arguments passed to the </span>
<span class="sd">            :func:`~giagrad.tensor.Function.forward` method of the </span>
<span class="sd">            ``fn`` parameter.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from giagrad.mlops import Softmax</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.27639335  0.7524293   0.69203097]</span>
<span class="sd">                 [ 0.37772807 -0.9291505  -0.80418533]]</span>
<span class="sd">        &gt;&gt;&gt; Tensor.comm(Softmax(axis=1), t)</span>
<span class="sd">        tensor: [[0.24242324 0.390224   0.36735278]</span>
<span class="sd">                 [0.6339727  0.17159334 0.19443396]] fn: Softmax(axis=1)</span>

<span class="sd">        .. _numpy.array: https://numpy.org/doc/stable/reference/generated/numpy.array.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">operands</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">operands</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">function</span><span class="p">)</span></div>
    

    <span class="c1"># ***** math functions (unary) ***** </span>
<div class="viewcode-block" id="Tensor.sqrt"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sqrt.html#giagrad.Tensor.sqrt">[docs]</a>    <span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the square-root of the elements </span>
<span class="sd">        of `data`.</span>
<span class="sd">        </span>
<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :func:`~giagrad.Tensor.pow`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.square"><a class="viewcode-back" href="../../generated/giagrad.Tensor.square.html#giagrad.Tensor.square">[docs]</a>    <span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the square of the elements </span>
<span class="sd">        of `data`. </span>
<span class="sd">        </span>
<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :func:`~giagrad.Tensor.pow`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.exp"><a class="viewcode-back" href="../../generated/giagrad.Tensor.exp.html#giagrad.Tensor.exp">[docs]</a>    <span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the exponential of the elements </span>
<span class="sd">        of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \exp^{data_i}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor([0, 0.6931471805599453]).exp()</span>
<span class="sd">        tensor: [1. 2.] fn: Exp</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Exp</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.log"><a class="viewcode-back" href="../../generated/giagrad.Tensor.log.html#giagrad.Tensor.log">[docs]</a>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the natural logarithm of the elements </span>
<span class="sd">        of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \log_e(data_i)</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(3).uniform() * 1e4</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [9553.524  3221.3936 6511.507 ] fn: Mul</span>
<span class="sd">        &gt;&gt;&gt; t.log()</span>
<span class="sd">        tensor: [7.650997 8.125444 8.514212] fn: Ln</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Log</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.reciprocal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.reciprocal.html#giagrad.Tensor.reciprocal">[docs]</a>    <span class="k">def</span> <span class="nf">reciprocal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the reciprocal of the elements </span>
<span class="sd">        of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{data_i}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(3).uniform() </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [0.00142364 0.8617358  0.30606526]</span>
<span class="sd">        &gt;&gt;&gt; t.reciprocal()</span>
<span class="sd">        tensor: [702.4239      1.1604484   3.267277 ] fn: Reciprocal</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Reciprocal</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.abs"><a class="viewcode-back" href="../../generated/giagrad.Tensor.abs.html#giagrad.Tensor.abs">[docs]</a>    <span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the absolute value of the elements </span>
<span class="sd">        of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \lvert data_i \rvert</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor([-1, -2, -3]).abs()</span>
<span class="sd">        tensor: [1. 2. 3.] fn: Abs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Abs</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span> </div>

    <span class="c1"># ***** math functions (binary) *****</span>
<div class="viewcode-block" id="Tensor.add"><a class="viewcode-back" href="../../generated/giagrad.Tensor.add.html#giagrad.Tensor.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the sum of `data` and ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object to add to `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.sub"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sub.html#giagrad.Tensor.sub">[docs]</a>    <span class="k">def</span> <span class="nf">sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the substraction of ``other`` </span>
<span class="sd">        from `data`.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object to substract from `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__sub__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.mul"><a class="viewcode-back" href="../../generated/giagrad.Tensor.mul.html#giagrad.Tensor.mul">[docs]</a>    <span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the multiplication of `data` </span>
<span class="sd">        to ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object that multiplies `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.pow"><a class="viewcode-back" href="../../generated/giagrad.Tensor.pow.html#giagrad.Tensor.pow">[docs]</a>    <span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with `data` raised to the power of ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object that `data` is raised to.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__pow__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.matmul"><a class="viewcode-back" href="../../generated/giagrad.Tensor.matmul.html#giagrad.Tensor.matmul">[docs]</a>    <span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the matrix multiplication </span>
<span class="sd">        of `data` and ``other``.</span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like </span>
<span class="sd">            The array_like object that `data` is multiplied to </span>
<span class="sd">            from the left-hand side.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__matmul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.div"><a class="viewcode-back" href="../../generated/giagrad.Tensor.div.html#giagrad.Tensor.div">[docs]</a>    <span class="k">def</span> <span class="nf">div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the division of `data` to ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object that divides `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__truediv__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="mf">0.0</span><span class="o">-</span><span class="bp">self</span> <span class="c1"># Tensor.comm(mops.Mul, self, -1)</span>
    <span class="c1"># TODO</span>
    <span class="k">def</span> <span class="nf">clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">):</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span> <span class="c1"># ((self-min_).relu()+min_) - (self-max_).relu()</span>
    <span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span> <span class="c1"># return self / (self.abs() + 1e-10)</span>

    <span class="c1"># ***** activation functions (unary) ***** </span>
<div class="viewcode-block" id="Tensor.relu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.relu.html#giagrad.Tensor.relu">[docs]</a>    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the Rectified Linear Unit (ReLU) function element-wise. </span>
<span class="sd">        See `ReLU`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \max(0, data)</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.96863234  0.64852756 -0.52318954]</span>
<span class="sd">                 [-0.18809071 -0.48402452  0.86754996]]</span>
<span class="sd">        &gt;&gt;&gt; t.relu()</span>
<span class="sd">        tensor: [[0.96863234 0.64852756 0.        ]</span>
<span class="sd">                 [0.         0.         0.86754996]] fn: ReLU</span>

<span class="sd">        .. _ReLU: https://paperswithcode.com/method/relu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.sigmoid"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sigmoid.html#giagrad.Tensor.sigmoid">[docs]</a>    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise sigmoid function. </span>
<span class="sd">        See `sigmoid`_.</span>

<span class="sd">        For numerical stability sigmoid function is computed with </span>
<span class="sd">        `numpy.logaddexp`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{(1 + \exp(-data_i))}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-100, 100)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-49.970577  35.522175 -14.944364]</span>
<span class="sd">                 [ 32.187164 -66.65264   48.01228 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.sigmoid()</span>
<span class="sd">        tensor: [[1.9863422e-22 1.0000000e+00 3.2340398e-07]</span>
<span class="sd">                 [1.0000000e+00 1.1301229e-29 1.0000000e+00]] fn: Sigmoid</span>

<span class="sd">        .. _numpy.logaddexp: https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html</span>
<span class="sd">        .. _sigmoid: https://paperswithcode.com/method/sigmoid-activation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.elu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.elu.html#giagrad.Tensor.elu">[docs]</a>    <span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Exponential Linear Unit (ELU) </span>
<span class="sd">        function to `data`. See `ELU`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i =</span>
<span class="sd">            \begin{cases} </span>
<span class="sd">                data_i \ \ if \ \ data_i &gt; 0 \\ </span>
<span class="sd">                \text{alpha}(\exp(data_i) - 1) \ \ if \ \ x \leq 0 \\</span>
<span class="sd">            \end{cases}</span>
<span class="sd">            </span>
<span class="sd">        .. _ELU: https://paperswithcode.com/method/elu</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        alpha: float</span>
<span class="sd">            The :math:`\alpha` value for the ELU formulation.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-100, 100)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-49.970577  35.522175 -14.944364]</span>
<span class="sd">                 [ 32.187164 -66.65264   48.01228 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.elu()</span>
<span class="sd">        tensor: [[-1.        35.522175  -0.9999997]</span>
<span class="sd">                 [32.187164  -1.        48.01228  ]] fn: ELU(alpha=1.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.silu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.silu.html#giagrad.Tensor.silu">[docs]</a>    <span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise Sigmoid-Weighted Linear </span>
<span class="sd">        Unit (SiLU) function, also called Swish. See `Swish`_.</span>
<span class="sd">    </span>
<span class="sd">        For numerical stability SiLU is computed with `numpy.logaddexp`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{data_i}{(1 + \exp(\text{beta} \times -data_i))} </span>
<span class="sd">        </span>
<span class="sd">        .. _Swish: https://paperswithcode.com/method/swish</span>
<span class="sd">        .. _numpy.logaddexp: https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        beta: float</span>
<span class="sd">            Hyperparameter for Swish formulation.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-10, 10)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 5.4958744   0.13549101 -4.5210676 ]</span>
<span class="sd">                 [-1.7155124   5.2369795  -7.6546626 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.silu()</span>
<span class="sd">        tensor: [[ 5.4734135e+00  7.2327957e-02 -4.8648320e-02]</span>
<span class="sd">                 [-2.6153007e-01  5.2092857e+00 -3.6252895e-03]] fn: SiLU(beta=1.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.tanh"><a class="viewcode-back" href="../../generated/giagrad.Tensor.tanh.html#giagrad.Tensor.tanh">[docs]</a>    <span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the Tanh function element-wise. See `Tanh`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{e^{data_i} - e^{-data_i}}{e^{data_i} + e^{-data_i}}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-8, 8)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-0.42122853 -3.4285958   7.846644  ]</span>
<span class="sd">                 [ 0.7483299   6.6553855   3.3439522 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.tanh()                                                                                             </span>
<span class="sd">        tensor: [[-0.3979649  -0.9978985   0.9999997 ]</span>
<span class="sd">                 [ 0.6341515   0.99999666  0.9975113 ]] fn: tanh</span>

<span class="sd">        .. _Tanh: https://paperswithcode.com/method/tanh-activation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.leakyrelu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.leakyrelu.html#giagrad.Tensor.leakyrelu">[docs]</a>    <span class="k">def</span> <span class="nf">leakyrelu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Leaky Rectified Linear Unit </span>
<span class="sd">        (Leaky ReLU) function to `data`. See `Leaky ReLU`_ .</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i =</span>
<span class="sd">            \begin{cases} </span>
<span class="sd">                data_i \ \ if \ \ data_i &gt; 0 \\ </span>
<span class="sd">                \text{neg_slope} \times data_i \ \ if \ \ x \leq 0 \\</span>
<span class="sd">            \end{cases}</span>
<span class="sd">        </span>
<span class="sd">        .. _Leaky ReLU: https://paperswithcode.com/method/elu</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        neg_slope: float</span>
<span class="sd">            Controls de angle of the negative slope (which only affects </span>
<span class="sd">            negative input values).</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, requires_grad=True).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-0.83589154  0.8874637  -0.465633  ]</span>
<span class="sd">                 [-0.5879877   0.22095676 -0.0592072 ]]</span>
<span class="sd">        &gt;&gt;&gt; d = t.leakyrelu(neg_slope=3)                                                                         </span>
<span class="sd">        &gt;&gt;&gt; d</span>
<span class="sd">        tensor: [[-2.5076747   0.8874637  -1.396899  ]</span>
<span class="sd">                 [-1.7639632   0.22095676 -0.17762159]] fn: LeakyReLU(neg_slope=3)</span>
<span class="sd">        &gt;&gt;&gt; d.backward()                                                                                         </span>
<span class="sd">        &gt;&gt;&gt; t.grad</span>
<span class="sd">        array([[3., 1., 3.],</span>
<span class="sd">               [3., 1., 3.]], dtype=float32)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">neg_slope</span><span class="o">=</span><span class="n">neg_slope</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.softplus"><a class="viewcode-back" href="../../generated/giagrad.Tensor.softplus.html#giagrad.Tensor.softplus">[docs]</a>    <span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mf">20.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the Softplus function element-wise. See `Softplus`_.</span>

<span class="sd">        For numerical stability the implementation reverts to the linear </span>
<span class="sd">        function when :math:`data_i \times \text{beta} &gt; \text{limit}`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{\text{beta}} \cdot \log(1 + \exp(\text{beta} \times data_i))</span>
<span class="sd">    </span>
<span class="sd">        .. _Softplus: https://paperswithcode.com/method/softplus</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        beta: float</span>
<span class="sd">            The :math:`\beta` value for the Softplus formulation.</span>
<span class="sd">        limit: float</span>
<span class="sd">            Data times beta above this reverts to a linear function.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t                                                                                                    </span>
<span class="sd">        tensor: [[ 0.54631704 -0.703394    0.85786563]</span>
<span class="sd">                 [-0.24458279  0.23733494 -0.32190484]]</span>
<span class="sd">        &gt;&gt;&gt; t.softplus(beta=5, limit=1)</span>
<span class="sd">        tensor: [[0.54631704 0.00585142 0.85786563]</span>
<span class="sd">                 [0.05160499 0.23733494 0.03646144]] fn: Softplus(lim=1, alpha=5)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Softplus</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.quick_gelu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.quick_gelu.html#giagrad.Tensor.quick_gelu">[docs]</a>    <span class="k">def</span> <span class="nf">quick_gelu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise Quick GELU. See `GELU`_.</span>
<span class="sd">        </span>
<span class="sd">        Quick GELU is an approximation of GELU through </span>
<span class="sd">        :func:`~giagrad.Tensor.silu` with alpha = 1.702 to </span>
<span class="sd">        ease GELU&#39;s computational complexity. </span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)                                                       </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.62271285  0.37412217 -0.6465454 ]</span>
<span class="sd">                 [-0.9013401  -0.02915052 -0.9814293 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.quick_gelu()</span>
<span class="sd">        tensor: [[ 0.4624659   0.2446833  -0.16141725]</span>
<span class="sd">                 [-0.15989538 -0.01421376 -0.15543076]] fn: QuickGELU</span>

<span class="sd">        .. _GELU: https://paperswithcode.com/method/gelu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">SiLU</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mf">1.702</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.gelu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.gelu.html#giagrad.Tensor.gelu">[docs]</a>    <span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Gaussina Error Linear Unit </span>
<span class="sd">        (Leaky ReLU) function to `data`. See `GELU`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i = data_i \ \Phi(data_i) </span>
<span class="sd">                = data_i \times \frac{1}{2} \left[1 + \text{erf}(\frac{data_i}{\sqrt{2}})\right]</span>
<span class="sd">        </span>
<span class="sd">        Where :math:`\Phi` is the Gaussian cumulative distribution function.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t                                                                                                    </span>
<span class="sd">        tensor: [[-0.42565832  0.8579072  -0.40772486]</span>
<span class="sd">                 [ 0.4038496   0.09953032 -0.6694602 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.gelu()                                                                                             </span>
<span class="sd">        tensor: [[-0.14268097  0.6901076  -0.1393431 ]</span>
<span class="sd">                 [ 0.26525608  0.05371065 -0.1684846 ]] fn: GELU</span>

<span class="sd">        .. _GELU: https://paperswithcode.com/method/gelu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.relu6"><a class="viewcode-back" href="../../generated/giagrad.Tensor.relu6.html#giagrad.Tensor.relu6">[docs]</a>    <span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies a modified version of ReLU with maximum size of 6. </span>
<span class="sd">        See `ReLU6`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \min(\max(0, x), 6)</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 20)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[11.792983   -0.20050316 15.441884  ]</span>
<span class="sd">                 [ 3.5337465  13.230399    9.813518  ]]</span>
<span class="sd">        &gt;&gt;&gt; t.relu6()</span>
<span class="sd">        tensor: [[6.        0.        6.       ]</span>
<span class="sd">                 [3.5337465 6.        6.       ]] fn: ReLU6</span>

<span class="sd">        .. _ReLU6: https://paperswithcode.com/method/relu6</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.mish"><a class="viewcode-back" href="../../generated/giagrad.Tensor.mish.html#giagrad.Tensor.mish">[docs]</a>    <span class="k">def</span> <span class="nf">mish</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mf">20.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise Mish function. </span>
<span class="sd">        See `Mish`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i = data_i \times \text{tanh} \left( \text{softplus}(data_i) \right)</span>
<span class="sd">        </span>
<span class="sd">        .. _Mish: https://paperswithcode.com/method/mish</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        beta: float</span>
<span class="sd">            The :math:`\beta` value for the Softplus formulation.</span>
<span class="sd">        limit: float</span>
<span class="sd">            Data times beta above limit reverts to a linear function in </span>
<span class="sd">            Softplus formulation.</span>
<span class="sd">        </span>
<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :func:`~giagrad.Tensor.softplus`</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-5, 5)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-1.3851491  1.2130666 -4.9049625]</span>
<span class="sd">                 [ 2.6859815 -4.845946   2.1385565]]</span>
<span class="sd">        &gt;&gt;&gt; t.mish()                                                                                             </span>
<span class="sd">        tensor: [[-0.3043592   1.0920179  -0.03620962]</span>
<span class="sd">                 [ 2.6642     -0.03794033  2.0915585 ]] fn: Mish(beta=1.0, lim=20.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Mish</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.hardswish"><a class="viewcode-back" href="../../generated/giagrad.Tensor.hardswish.html#giagrad.Tensor.hardswish">[docs]</a>    <span class="k">def</span> <span class="nf">hardswish</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Hard Swish function to `data`. </span>
<span class="sd">        See `Hard Swish`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i = data_i \, \times \, \frac{\text{ReLU6}(data_i + 3)}{6}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 4).uniform(-5, 5)                                                                 </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-4.0175104   3.993501   -1.0318986  -0.30065283]</span>
<span class="sd">                 [-2.4765007  -1.3878915   1.7888396   4.3194094 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.hardswish()                                                                 </span>
<span class="sd">        tensor: [[-0.          3.993501   -0.3384802  -0.13526106]</span>
<span class="sd">                 [-0.21607438 -0.37290528  1.4277442   4.3194094 ]] fn: Hardswish</span>
<span class="sd">        </span>
<span class="sd">        .. _Hard Swish: https://paperswithcode.com/method/hard-swish</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Hardswish</span><span class="p">(),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.softmax"><a class="viewcode-back" href="../../generated/giagrad.Tensor.softmax.html#giagrad.Tensor.softmax">[docs]</a>    <span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies Softmax function to every 1-D slice defined by ``axis``. </span>
<span class="sd">        See `Softmax`_.</span>

<span class="sd">        The elements of the n-dimensinal output Tensor will lie in the </span>
<span class="sd">        range :math:`[0, 1]` and sum to :math:`1` for the specified 1-D </span>
<span class="sd">        slices defined by ``axis``.</span>
<span class="sd">        </span>
<span class="sd">        Softmax for a one-dimensional slice is defined as:</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            \text{Softmax}(x_i) = \frac{\exp(x_i)}{\sum_j \exp(x_j)} </span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: int</span>
<span class="sd">            The dimension along which Softmax will be computed </span>
<span class="sd">            (so every slice along axis will sum to 1).</span>


<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.27639335  0.7524293   0.69203097]</span>
<span class="sd">                 [ 0.37772807 -0.9291505  -0.80418533]]</span>
<span class="sd">        &gt;&gt;&gt; t.softmax(axis=1)</span>
<span class="sd">        tensor: [[0.24242324 0.390224   0.36735278]</span>
<span class="sd">                 [0.6339727  0.17159334 0.19443396]] fn: Softmax(axis=1)</span>

<span class="sd">        .. _Softmax: https://paperswithcode.com/method/softmax</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.log_softmax"><a class="viewcode-back" href="../../generated/giagrad.Tensor.log_softmax.html#giagrad.Tensor.log_softmax">[docs]</a>    <span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies LogSoftmax function to every 1-D slice defined by ``axis``.</span>

<span class="sd">        LogSoftmax for a one-dimensional slice is defined as:</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            \text{LogSoftmax}(x_i) = \log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)} \right)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: int</span>
<span class="sd">            The dimension along which LogSoftmax will be computed.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-0.07469178  0.7226724   0.98966014]</span>
<span class="sd">                 [-0.01990889 -0.4521888   0.26520386]]</span>
<span class="sd">        &gt;&gt;&gt; t.softmax(axis=1)</span>
<span class="sd">        tensor: [[-0.72091377 -0.26915795 -0.39513725]</span>
<span class="sd">                 [-0.6661309  -1.4440191  -1.1195936 ]] fn: LogSoftmax(axis=0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

    <span class="c1"># ***** math functions (binary) *****</span>
    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Add</span><span class="p">(),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Add</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Sub</span><span class="p">(),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Sub</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Mul</span><span class="p">(),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Mul</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Pow</span><span class="p">(),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Pow</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Div</span><span class="p">(),</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> 
    <span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Div</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> 

    <span class="c1"># ***** math functions autossign (i.e. a += b) *****</span>
    <span class="k">def</span> <span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">**=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__imatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>

    <span class="c1"># ***** logical *****</span>
    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span>  <span class="o">&lt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>  
    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span>  <span class="o">&gt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>  
    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>   
    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">==</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>        
    <span class="k">def</span> <span class="fm">__ne__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">!=</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>       
    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">&gt;=</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>   
    <span class="c1"># need __hash__ due to __eq__</span>
    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="nb">hash</span><span class="p">((</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
    <span class="c1"># ***** math functions (reduction) *****</span>
<div class="viewcode-block" id="Tensor.mean"><a class="viewcode-back" href="../../generated/giagrad.Tensor.mean.html#giagrad.Tensor.mean">[docs]</a>    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the mean value of each 1-D slice of the tensor in the </span>
<span class="sd">        given ``axis``, if ``axis`` is a list of dimensions, reduce </span>
<span class="sd">        over all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as </span>
<span class="sd">        input except in the ``axis`` where it is of size 1. Otherwise, </span>
<span class="sd">        every ``axis`` is squeezed, leading to an output tensor with </span>
<span class="sd">        fewer dimensions. If no ``axis`` is supplied all data is </span>
<span class="sd">        reduced to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, mean reduces </span>
<span class="sd">            all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor(np.arange(12).reshape((2,2,3)))</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[ 0.  1.  2.]</span>
<span class="sd">                  [ 3.  4.  5.]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 6.  7.  8.]</span>
<span class="sd">                  [ 9. 10. 11.]]]</span>
<span class="sd">        &gt;&gt;&gt; t.mean(axis=(0, 1), keepdims=True)                                      </span>
<span class="sd">        tensor: [[[4.5 5.5 6.5]]] fn: Mean(axis=(0, 1))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.sum"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sum.html#giagrad.Tensor.sum">[docs]</a>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the sum of each 1-D slice of the tensor in the given </span>
<span class="sd">        ``axis``, if ``axis`` is a list of dimensions, reduce over all </span>
<span class="sd">        of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as </span>
<span class="sd">        input except in the ``axis`` where it is of size 1. Otherwise, </span>
<span class="sd">        every ``axis`` is squeezed, leading to an output tensor with </span>
<span class="sd">        fewer dimensions. If no ``axis`` is supplied all data is reduced</span>
<span class="sd">        to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, sum reduces </span>
<span class="sd">            all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, 4, dtype=int).uniform(0, 5)                          </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[2 0 0 3]</span>
<span class="sd">                  [0 2 1 4]</span>
<span class="sd">                  [4 0 0 2]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[3 1 4 0]</span>
<span class="sd">                  [3 3 4 3]</span>
<span class="sd">                  [4 0 1 0]]]</span>
<span class="sd">        &gt;&gt;&gt; t.sum(axis=2, keepdims=True)                           </span>
<span class="sd">        tensor: [[[ 5.]</span>
<span class="sd">                  [ 7.]</span>
<span class="sd">                  [ 6.]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 8.]</span>
<span class="sd">                  [13.]</span>
<span class="sd">                  [ 5.]]] fn: Sum(axis=2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">Sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.max"><a class="viewcode-back" href="../../generated/giagrad.Tensor.max.html#giagrad.Tensor.max">[docs]</a>    <span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the maximum value of each 1-D slice of the tensor in the </span>
<span class="sd">        given ``axis``, if ``axis`` is a list of dimensions, reduce over </span>
<span class="sd">        all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as </span>
<span class="sd">        input except in the ``axis`` where it is of size 1. Otherwise, </span>
<span class="sd">        every ``axis`` is squeezed, leading to an output tensor with </span>
<span class="sd">        fewer dimensions. If no ``axis`` is supplied all data is reduced</span>
<span class="sd">        to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, max reduces </span>
<span class="sd">            all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, 4, dtype=np.int8).uniform(0, 100)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[54 83 83 67]</span>
<span class="sd">                  [81 64 76 51]</span>
<span class="sd">                  [76 98 58 28]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[64 91 59 48]</span>
<span class="sd">                  [70 41 16 33]</span>
<span class="sd">                  [27 44 17 70]]]</span>
<span class="sd">        &gt;&gt;&gt; t.max(axis=(1, 2))                                                          </span>
<span class="sd">        tensor: [98. 91.] fn: Max(axis=(1, 2))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">MinMax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.min"><a class="viewcode-back" href="../../generated/giagrad.Tensor.min.html#giagrad.Tensor.min">[docs]</a>    <span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the minimum value of each 1-D slice of the tensor in the </span>
<span class="sd">        given ``axis``, if ``axis`` is a list of dimensions, reduce over </span>
<span class="sd">        all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as </span>
<span class="sd">        input except in the ``axis`` where it is of size 1. Otherwise, </span>
<span class="sd">        every ``axis`` is squeezed, leading to an output tensor with </span>
<span class="sd">        fewer dimensions. If no ``axis`` is supplied all data is reduced </span>
<span class="sd">        to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, min reduces </span>
<span class="sd">            all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, 4, dtype=np.int8).uniform(0, 20)                     </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[ 3 14 15  7]</span>
<span class="sd">                  [18  9 11 18]</span>
<span class="sd">                  [16 17 14  9]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 5  3 12 18]</span>
<span class="sd">                  [15 11 15  1]</span>
<span class="sd">                  [13  2  2 10]]]</span>
<span class="sd">        &gt;&gt;&gt; t.min(axis=2, keepdims=True)                                                </span>
<span class="sd">        tensor: [[[3.]</span>
<span class="sd">                  [9.]</span>
<span class="sd">                  [9.]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[3.]</span>
<span class="sd">                  [1.]</span>
<span class="sd">                  [2.]]] fn: Min(axis=2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">MinMax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

    <span class="c1"># ***** shape functions *****</span>
<div class="viewcode-block" id="Tensor.permute"><a class="viewcode-back" href="../../generated/giagrad.Tensor.permute.html#giagrad.Tensor.permute">[docs]</a>    <span class="k">def</span> <span class="nf">permute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a view of the original tensor with its ``axes`` permuted.</span>

<span class="sd">        ``permute`` uses `numpy.transpose`_, the following documentation </span>
<span class="sd">        is adapted.</span>

<span class="sd">        For a 1-D tensor, this returns an unchanged view of the original </span>
<span class="sd">        tensor, as a transposed vector is simply the same vector. </span>
<span class="sd">        To convert a 1-D tensor into a 2-D tensor vector, an additional </span>
<span class="sd">        dimension must be added, e.g., tensor.unsqueeze(axis=0) achieves </span>
<span class="sd">        this, as does Tensor[:, None]. </span>

<span class="sd">        For a 2-D tensor, this is the standard matrix transpose. For an </span>
<span class="sd">        n-D tensor, if axes are given, their order indicates how the </span>
<span class="sd">        axes are permuted (see Examples). If axes are not provided, then </span>
<span class="sd">        tensor.permute().shape == tensor.shape[::-1].</span>

<span class="sd">        </span>
<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :meth:`giagrad.Tensor.swapaxes`</span>
<span class="sd">            For swaping only two axes.</span>
<span class="sd">        :func:`numpy.transpose`</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axes: tuple or list of ints, optional</span>
<span class="sd">            If specified, it must be a tuple or list which contains a </span>
<span class="sd">            permutation of [0,1,...,N-1] where N is the number of axes </span>
<span class="sd">            of the original tensor. The **i**th axis of the returned </span>
<span class="sd">            tensor will correspond to the axis numbered ``axes[i]`` of </span>
<span class="sd">            the input. </span>
<span class="sd">            If not specified, defaults to ``range(tensor.ndim)[::-1]``, </span>
<span class="sd">            which reverses the order of the axes.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(1, 2, 3, 2, dtype=int).uniform(-5, 5)                    </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[[ 1  0]</span>
<span class="sd">                   [-3  4]</span>
<span class="sd">                   [ 3  3]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[ 3 -4]</span>
<span class="sd">                   [-3  1]</span>
<span class="sd">                   [-2  3]]]]</span>
<span class="sd">        </span>
<span class="sd">        Note that axes has the same lenght as :attr:`giagrad.Tensor.ndim`.</span>

<span class="sd">        &gt;&gt;&gt; t.permute(axes=(1, 2, 3, 0))                                              </span>
<span class="sd">        tensor: [[[[ 1]</span>
<span class="sd">                   [ 0]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[-3]</span>
<span class="sd">                   [ 4]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[ 3]</span>
<span class="sd">                   [ 3]]]</span>
<span class="sd">        ...</span>
<span class="sd">        ...</span>
<span class="sd">                 [[[ 3]</span>
<span class="sd">                   [-4]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[-3]</span>
<span class="sd">                   [ 1]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[-2]</span>
<span class="sd">                   [ 3]]]] fn: Permute(axes = (1, 2, 3, 0))</span>
<span class="sd">        &gt;&gt;&gt; t.permute(axes=(1, 2, 3, 0)).shape</span>
<span class="sd">        (2, 3, 2, 1)</span>

<span class="sd">        .. _numpy.transpose: https://numpy.org/doc/stable/reference/generated/numpy.transpose.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Permute</span><span class="p">(</span><span class="n">axes</span><span class="o">=</span><span class="n">axes</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.swapaxes"><a class="viewcode-back" href="../../generated/giagrad.Tensor.swapaxes.html#giagrad.Tensor.swapaxes">[docs]</a>    <span class="k">def</span> <span class="nf">swapaxes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Permutes two specific axes.</span>

<span class="sd">        Note</span>
<span class="sd">        ----</span>
<span class="sd">        The returned tensor shares the storage with the input tensor, </span>
<span class="sd">        so changing the contents of one will change the contents of the </span>
<span class="sd">        other.</span>
<span class="sd">        </span>
<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :attr:`giagrad.Tensor.T`, :meth:`giagrad.Tensor.permute`</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis0: int</span>
<span class="sd">            First axis.</span>
<span class="sd">        axis1: int</span>
<span class="sd">            Second axis.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(1, 2, 3, 2, dtype=int).uniform(-100, 100)                </span>
<span class="sd">        &gt;&gt;&gt; t                                                                         </span>
<span class="sd">        tensor: [[[[-91  22]</span>
<span class="sd">                   [ 54 -47]</span>
<span class="sd">                   [ 21 -88]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[  3 -78]</span>
<span class="sd">                   [ 34  68]</span>
<span class="sd">                   [-51  29]]]]</span>
<span class="sd">        &gt;&gt;&gt; t.swapaxes(2, 3)                                                           </span>
<span class="sd">        tensor: [[[[-91  54  21]</span>
<span class="sd">                   [ 22 -47 -88]]</span>
<span class="sd">        ...</span>
<span class="sd">                  [[  3  34 -51]</span>
<span class="sd">                   [-78  68  29]]]] fn: Swapaxes(2, 3)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Swapaxes</span><span class="p">(</span><span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;Returns a transposed view of a 2 dimensional Tensor.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Dimensions = 2 required, this is matrix transposition&quot;</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Getitem</span><span class="p">(</span><span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.pad"><a class="viewcode-back" href="../../generated/giagrad.Tensor.pad.html#giagrad.Tensor.pad">[docs]</a>    <span class="k">def</span> <span class="nf">pad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pads tensor.</span>

<span class="sd">        Padding size specified by ``*padding`` maps every argument </span>
<span class="sd">        starting from the rightmost axis. If ``*padding`` is a single </span>
<span class="sd">        int ``N`` it will be interpreted as if &quot;before&quot; and &quot;after&quot; </span>
<span class="sd">        padding for the last axis is symmetric, i.e. ``(N_before, N_after)``. </span>
<span class="sd">        If a tuple of two integers is supplied, it will be interpreted </span>
<span class="sd">        as ``(N_before, N_after)`` padding.</span>

<span class="sd">        Padding ``mode`` has the same options as `numpy.pad`_.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :func:`numpy.pad`</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *padding: int or (int, int)</span>
<span class="sd">            Number of values padded to the edges of the rightmost axes.</span>
<span class="sd">        mode: str, default: &#39;constant&#39;</span>
<span class="sd">            Padding mode defined by `numpy.pad`_.</span>
<span class="sd">        **kwargs:</span>
<span class="sd">            Optional arguments passed to `numpy.pad`_.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 2, 3, dtype=int).uniform(-5, 5)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[ 0 -1  0]</span>
<span class="sd">                  [ 0  2  0]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[-1 -2  0]</span>
<span class="sd">                  [-3 -1  3]]]</span>
<span class="sd">        </span>
<span class="sd">        A single int padds the last axis with before and after </span>
<span class="sd">        symmetrically:</span>

<span class="sd">        &gt;&gt;&gt; t.pad(2)</span>
<span class="sd">        tensor: [[[ 0  0  0 -1  0  0  0]</span>
<span class="sd">                  [ 0  0  0  2  0  0  0]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 0  0 -1 -2  0  0  0]</span>
<span class="sd">                  [ 0  0 -3 -1  3  0  0]]] fn: ConstantPad</span>
<span class="sd">        &gt;&gt;&gt; t.pad((1, 0), 2, (1, 3))</span>
<span class="sd">        tensor: [[[ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0 -1  0  0  0  0]</span>
<span class="sd">                  [ 0  0  2  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0 -1 -2  0  0  0  0]</span>
<span class="sd">                  [ 0 -3 -1  3  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]</span>
<span class="sd">                  [ 0  0  0  0  0  0  0]]] fn: ConstantPad</span>
<span class="sd">        </span>
<span class="sd">        .. _numpy.pad: </span>
<span class="sd">            https://numpy.org/doc/stable/reference/generated/numpy.pad.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_tup</span> <span class="o">=</span> <span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">i</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="k">else</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">all_tup</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Pad</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.squeeze"><a class="viewcode-back" href="../../generated/giagrad.Tensor.squeeze.html#giagrad.Tensor.squeeze">[docs]</a>    <span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove axes of length one.</span>

<span class="sd">        For example, a tensor of shape :math:`(1, N_1, N_2, 1, N_3, 1)` </span>
<span class="sd">        will be reshaped into :math:`(N_1, N_2, N_3)` if no axis is </span>
<span class="sd">        inputed. Specific axis with length one can be removed either </span>
<span class="sd">        passing an int or a tuple or ints.</span>
<span class="sd">        </span>
<span class="sd">        Note</span>
<span class="sd">        ----</span>
<span class="sd">        The returned tensor shares the storage with the input tensor, so </span>
<span class="sd">        changing the contents of one will change the contents of the other.</span>

<span class="sd">        Warning</span>
<span class="sd">        -------</span>
<span class="sd">        If the tensor has a batch dimension of size 1, then ``squeeze`` </span>
<span class="sd">        will also remove the batch dimension, which can lead to unexpected </span>
<span class="sd">        errors. Consider specifying only the dims you wish to be squeezed.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :meth:`giagrad.Tensor.unsqueeze`</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or int, optional</span>
<span class="sd">            By default removes all axes of length one, if tuple or int </span>
<span class="sd">            passed those axes will be removed.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(1, 2, 1, 2, 1).uniform()                                             </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[[[0.16217469]</span>
<span class="sd">                    [0.8090288 ]]]</span>
<span class="sd">        ...</span>
<span class="sd">        ...</span>
<span class="sd">                  [[[0.7216649 ]</span>
<span class="sd">                    [0.6690301 ]]]]]</span>
<span class="sd">        &gt;&gt;&gt; t.squeeze()</span>
<span class="sd">        tensor: [[0.16217469 0.8090288 ]</span>
<span class="sd">                 [0.7216649  0.6690301 ]] fn: Squeeze(axis = None)</span>
<span class="sd">        &gt;&gt;&gt; t.squeeze().shape</span>
<span class="sd">        (2, 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unsqueeze"><a class="viewcode-back" href="../../generated/giagrad.Tensor.unsqueeze.html#giagrad.Tensor.unsqueeze">[docs]</a>    <span class="k">def</span> <span class="nf">unsqueeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with its shape expanded.</span>

<span class="sd">        ``unsqueeze`` inserts a new axis of size one in the specified </span>
<span class="sd">        ``axis``. For example a tensor of shape :math:`(N_1, N_2, N_3)` </span>
<span class="sd">        with :math:`\text{axis}=(0, 2)` will output a tensor of shape </span>
<span class="sd">        :math:`(1, N_1, 1, N_2, N_3)`.</span>

<span class="sd">        Note</span>
<span class="sd">        ----</span>
<span class="sd">        The returned tensor shares the storage with the input tensor, </span>
<span class="sd">        so changing the contents of one will change the contents of the </span>
<span class="sd">        other.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        :meth:`giagrad.Tensor.squeeze`</span>
<span class="sd">        :func:`numpy.expand_dims`</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 2, 2).uniform()                                                   </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[0.54203224 0.4911729 ]</span>
<span class="sd">                  [0.29304293 0.9672827 ]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[0.18163016 0.34806943]</span>
<span class="sd">                  [0.30323076 0.3647484 ]]]</span>
<span class="sd">        &gt;&gt;&gt; t.unsqueeze(axis=(0,2))                                                               </span>
<span class="sd">        tensor: [[[[[0.54203224 0.4911729 ]</span>
<span class="sd">                    [0.29304293 0.9672827 ]]]</span>
<span class="sd">        ...</span>
<span class="sd">        ...</span>
<span class="sd">                  [[[0.18163016 0.34806943]</span>
<span class="sd">                    [0.30323076 0.3647484 ]]]]] fn: UnSqueeze(axis = (0, 2))</span>
<span class="sd">        &gt;&gt;&gt; t.unsqueeze(axis=(0,2)).shape</span>
<span class="sd">        (1, 2, 1, 2, 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">UnSqueeze</span><span class="p">(</span><span class="n">axis</span><span class="p">),</span> <span class="bp">self</span><span class="p">)</span></div>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner"></div>
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
       Copyright 2023, Carlos Prez.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://sphinx-doc.org/">Sphinx</a> 6.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>