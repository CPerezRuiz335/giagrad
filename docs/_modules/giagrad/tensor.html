

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>giagrad.tensor &#8212; giagrad  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx_paramlinks.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/giagrad/tensor';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">giagrad  documentation</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor.html">
                        Core
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/CPerezRuiz335/giagrad" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tensor.html">
                        Core
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/CPerezRuiz335/giagrad" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">giagrad.tensor</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for giagrad.tensor</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">numpy.typing</span> <span class="kn">import</span> <span class="n">NDArray</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="k">class</span> <span class="nc">Context</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_for_backward</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parents</span> <span class="o">=</span> <span class="n">save_for_backward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="Context.forward"><a class="viewcode-back" href="../../generated/giagrad.tensor.Context.forward.html#giagrad.Context.forward">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">NDArray</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="n">Context</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Makes forward pass.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *tensors: Tensor</span>
<span class="sd">            A variable number of tensors, e.g. two for binary operators </span>
<span class="sd">            such as :func:`~giagrad.Tensor.matmul`.</span>

<span class="sd">        *kwargs: </span>
<span class="sd">            Optional arguments if needed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ndarray or float:</span>
<span class="sd">            The result of applying that operator to :paramref:`*tensors`&#39;s data,</span>
<span class="sd">            i.g. float for reduction operators in some cases or a new ndarray.</span>
<span class="sd">        Context:</span>
<span class="sd">            This instance is the one defining the context of the child tensor </span>
<span class="sd">            created in :func:`~giagrad.Tensor.comm`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;forward not implemented for </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Context.backward"><a class="viewcode-back" href="../../generated/giagrad.tensor.Context.backward.html#giagrad.Context.backward">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partial</span><span class="p">:</span> <span class="n">NDArray</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Backpropagate from child tensor created with :func:`~giagrad.Tensor.comm`.</span>
<span class="sd">        </span>
<span class="sd">        Updates :attr:`~parents` gradient through chain rule. This method is the </span>
<span class="sd">        extension of :func:`~giagrad.Tensor.backward` for a concrete operator.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        partial: ndarray</span>
<span class="sd">            Defines the partial derivative of the loss function with respect to the </span>
<span class="sd">            child Tensor, the one created with :func:`~giagrad.tensor.Context.forward`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;backward not implemented for </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Context.__str__"><a class="viewcode-back" href="../../generated/giagrad.tensor.Context.__str__.html#giagrad.Context.__str__">[docs]</a>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default representation if :attr:`~_name` is not overriden.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;__str__ not implemented for class </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

<span class="kn">import</span> <span class="nn">giagrad.shapeops</span> <span class="k">as</span> <span class="nn">sops</span>
<span class="kn">import</span> <span class="nn">giagrad.mathops</span> <span class="k">as</span> <span class="nn">mops</span>
<span class="kn">import</span> <span class="nn">giagrad.reductionops</span> <span class="k">as</span> <span class="nn">rops</span>
<span class="kn">import</span> <span class="nn">giagrad.mlops</span> <span class="k">as</span> <span class="nn">mlops</span>
<span class="kn">import</span> <span class="nn">giagrad.initializers</span> <span class="k">as</span> <span class="nn">init</span>

<span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="n">__array_ufunc__</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># tell numpy to trust Tensor to make __r***__ method</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">,</span> <span class="s2">&quot;_ctx&quot;</span><span class="p">,</span> <span class="s2">&quot;requires_grad&quot;</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> 
            <span class="n">data</span><span class="p">,</span> 
            <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
            <span class="n">context</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Context</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
            <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span> <span class="o">=</span> <span class="n">context</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    
    <span class="c1"># ***** backprop *****</span>
<div class="viewcode-block" id="Tensor.backward"><a class="viewcode-back" href="../../generated/giagrad.Tensor.backward.html#giagrad.Tensor.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the gradient of all preceeding tensors.</span>
<span class="sd">        </span>
<span class="sd">        The graph is differentiated using the chain rule. Whether it is scalar </span>
<span class="sd">        or non-scalar (i.e. its data has more than one element), gradient is set</span>
<span class="sd">        to ones and backpropagated.</span>

<span class="sd">        This function accumulates gradients in every preceeding tensor, you might </span>
<span class="sd">        need to zero .grad attributes or set them to None before calling it. </span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        retain_graph: bool, default: False</span>
<span class="sd">            If ``False`` the graph used to compute the grads will be freed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>    
        <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">:=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">_ctx</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">context</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                        <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                        <span class="n">build_topo</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="n">topo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

        <span class="n">build_topo</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="c1"># chain rule </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># dL/dL = 1</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">tensor</span><span class="o">.</span><span class="n">_ctx</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">retain_graph</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span> <span class="o">=</span> <span class="kc">None</span> </div>

    <span class="c1"># ***** helpers *****</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tuple of tensor dimensions.</span>

<span class="sd">        Unlike numpy.ndarray.shape it can not be used to </span>
<span class="sd">        reshape inplace.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
    
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">type</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Data-type of the tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Size of the tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Number of dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">ndim</span>

<div class="viewcode-block" id="Tensor.no_grad"><a class="viewcode-back" href="../../generated/giagrad.Tensor.no_grad.html#giagrad.Tensor.no_grad">[docs]</a>    <span class="k">def</span> <span class="nf">no_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;Makes tensor autodifferentiable.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.requires_grad_"><a class="viewcode-back" href="../../generated/giagrad.Tensor.requires_grad_.html#giagrad.Tensor.requires_grad_">[docs]</a>    <span class="k">def</span> <span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Makes tensor not autodifferentiable.&quot;&quot;&quot;</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;tensor: &#39;</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array2string</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
            <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;tensor: &#39;</span><span class="p">)</span> \
            <span class="o">+</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot; grad_fn: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ctx</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> \
            <span class="o">+</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot;, name: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>

    <span class="c1"># ***** initializers in-place*****</span>
<div class="viewcode-block" id="Tensor.empty"><a class="viewcode-back" href="../../generated/giagrad.Tensor.empty.html#giagrad.Tensor.empty">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">empty</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a tensor filled with uninitialized data. </span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        shape: int, ...</span>
<span class="sd">            A variable number of integers defining the shape of the output tensor.</span>
<span class="sd">        \*\*kwargs:</span>
<span class="sd">            Parameters passed to the Tensor class initializer.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3, requires_grad=True, dtype=np.float64)</span>
<span class="sd">        tensor: [[4.67662529e-310 0.00000000e+000 4.67596337e-310]</span>
<span class="sd">                 [6.94592882e-310 6.94611561e-310 6.94609055e-310]]    </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="c1"># in-place initializers</span>
<div class="viewcode-block" id="Tensor.zeros"><a class="viewcode-back" href="../../generated/giagrad.Tensor.zeros.html#giagrad.Tensor.zeros">[docs]</a>    <span class="k">def</span> <span class="nf">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with zeros. </span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        -------_</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3).zeros()                                                                                           </span>
<span class="sd">        tensor: [[0. 0. 0.]</span>
<span class="sd">                 [0. 0. 0.]]  </span>
<span class="sd">        &gt;&gt;&gt; Tensor([1, 3, 4, 5]).zeros()</span>
<span class="sd">        tensor: [0., 0., 0., 0.] </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.ones"><a class="viewcode-back" href="../../generated/giagrad.Tensor.ones.html#giagrad.Tensor.ones">[docs]</a>    <span class="k">def</span> <span class="nf">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with ones. </span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3).ones()                                                                                           </span>
<span class="sd">        tensor: [[1. 1. 1.]</span>
<span class="sd">                 [1. 1. 1.]]  </span>
<span class="sd">        &gt;&gt;&gt; Tensor([1, 3, 4, 5]).ones()</span>
<span class="sd">        tensor: [1., 1., 1., 1.] </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.constant"><a class="viewcode-back" href="../../generated/giagrad.Tensor.constant.html#giagrad.Tensor.constant">[docs]</a>    <span class="k">def</span> <span class="nf">constant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with a constant value. </span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        fill_value: float</span>
<span class="sd">            The value to fill the tensor with.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(2, 3).constant(2.71828)                                                                                           </span>
<span class="sd">        tensor: [[2.71828 2.71828 2.71828]</span>
<span class="sd">                 [2.71828 2.71828 2.71828]]  </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="n">fill_value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
        
<div class="viewcode-block" id="Tensor.normal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.normal.html#giagrad.Tensor.normal">[docs]</a>    <span class="k">def</span> <span class="nf">normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills tensor data with values drawn from the normal</span>
<span class="sd">        distribution :math:`\mathcal{N}(\text{mu}, \text{std}^2)`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        mu: float</span>
<span class="sd">            Mean of the normal distribution.</span>
<span class="sd">        std: float</span>
<span class="sd">            The standard deviation of the normal distribution.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 3).normal()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.uniform"><a class="viewcode-back" href="../../generated/giagrad.Tensor.uniform.html#giagrad.Tensor.uniform">[docs]</a>    <span class="k">def</span> <span class="nf">uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with values drawn from the uniform</span>
<span class="sd">        distribution :math:`\mathcal{U}(a, b)`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        a: float</span>
<span class="sd">            The lower bound of the uniform distribution.</span>
<span class="sd">        b: float</span>
<span class="sd">            The upper bound of the uniform distribution.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 3).uniform()</span>
<span class="sd">        &quot;&quot;&quot;</span> 
        <span class="n">init</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.dirac"><a class="viewcode-back" href="../../generated/giagrad.Tensor.dirac.html#giagrad.Tensor.dirac">[docs]</a>    <span class="k">def</span> <span class="nf">dirac</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills the {3, 4, 5}-dimensional Tensor data with the Dirac delta function. </span>

<span class="sd">        Preserves the identity of the inputs in *Convolutional*</span>
<span class="sd">        layers, where as many input channels are preserved as possible. In case</span>
<span class="sd">        of groups &gt; 1, each group of channels preserves identity.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        groups: int, default: 1</span>
<span class="sd">            Number of groups in the conv layer.</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 24, 5, 5).dirac(3)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">dirac</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.xavier_uniform"><a class="viewcode-back" href="../../generated/giagrad.Tensor.xavier_uniform.html#giagrad.Tensor.xavier_uniform">[docs]</a>    <span class="k">def</span> <span class="nf">xavier_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known Glorot uniform initialization.</span>

<span class="sd">        This methos is described in `Understanding the difficulty of training deep feedforward</span>
<span class="sd">        neural networks` - Glorot, X. &amp; Bengio, Y. (2010), using a uniform</span>
<span class="sd">        distribution. Tensor data will have values sampled from :math:`\mathcal{U}(-a, a)` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            a = \text{gain} \times \sqrt{\frac{6}{\text{fan_in} + \text{fan_out}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gain: float</span>
<span class="sd">            An optional scaling factor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from giagrad import calculate_gain</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).xavier_uniform(gain=calculate_gain(&#39;relu&#39;))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
    
<div class="viewcode-block" id="Tensor.xavier_normal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.xavier_normal.html#giagrad.Tensor.xavier_normal">[docs]</a>    <span class="k">def</span> <span class="nf">xavier_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known Glorot normal initialization.</span>

<span class="sd">        This methos is described in `Understanding the difficulty of training deep feedforward</span>
<span class="sd">        neural networks` - Glorot, X. &amp; Bengio, Y. (2010), using a normal distribution. </span>
<span class="sd">        Tensor data will have values sampled from :math:`\mathcal{N}(0, \sigma^2)` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \sigma = \text{gain} \times \sqrt{\frac{2}{\text{fan_in} + \text{fan_out}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gain: float</span>
<span class="sd">            An optional scaling factor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from giagrad import calculate_gain</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).xavier_normal(gain=calculate_gain(&#39;relu&#39;))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">xavier_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>    </div>

<div class="viewcode-block" id="Tensor.kaiming_uniform"><a class="viewcode-back" href="../../generated/giagrad.Tensor.kaiming_uniform.html#giagrad.Tensor.kaiming_uniform">[docs]</a>    <span class="k">def</span> <span class="nf">kaiming_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known He uniform initialization.</span>

<span class="sd">        Tensor data is filled with values according to the method described </span>
<span class="sd">        in `Delving deep into rectifiers`_ using uniform distribution. The </span>
<span class="sd">        resulting tensor will have values sampled from</span>
<span class="sd">        :math:`\mathcal{U}(-\text{bound}, \text{bound})` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan_mode}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        neg_slope: float</span>
<span class="sd">            The negative slope of the rectifier used after this layer (only</span>
<span class="sd">            used with `&#39;leaky_relu&#39;`).</span>
<span class="sd">        mode: str, default: &#39;fan_in&#39;</span>
<span class="sd">            Either `&#39;fan_in&#39;` or `&#39;fan_out&#39;`. Choosing `&#39;fan_in&#39;`</span>
<span class="sd">            preserves the magnitude of the variance of the weights in the</span>
<span class="sd">            forward pass. Choosing `&#39;fan_out&#39;` preserves the magnitudes in the</span>
<span class="sd">            backwards pass.</span>
<span class="sd">        nonlinearity: str, default: &#39;leaky_relu&#39;</span>
<span class="sd">            The non-linear function method name, recommended to use only with </span>
<span class="sd">            `&#39;relu&#39;` or `&#39;leaky_relu&#39;`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).kaiming_uniform(mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)</span>

<span class="sd">        .. _Delving deep into rectifiers: https://arxiv.org/abs/1502.01852</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.kaiming_normal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.kaiming_normal.html#giagrad.Tensor.kaiming_normal">[docs]</a>    <span class="k">def</span> <span class="nf">kaiming_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_in&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with the also known He normal initialization.</span>

<span class="sd">        Tensor data is filled with values according to the method described </span>
<span class="sd">        in `Delving deep into rectifiers`_ using normal distribution. The </span>
<span class="sd">        resulting tensor will have values sampled from</span>
<span class="sd">        :math:`\mathcal{N}(0, \sigma^2)` where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \sigma = \frac{\text{gain}}{\sqrt{\text{fan_mode}}}</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        neg_slope: float</span>
<span class="sd">            The negative slope of the rectifier used after this layer (only</span>
<span class="sd">            used with `&#39;leaky_relu&#39;`).</span>
<span class="sd">        mode: str, default: &#39;fan_in&#39;</span>
<span class="sd">            Either `&#39;fan_in&#39;` or `&#39;fan_out&#39;`. Choosing `&#39;fan_in&#39;`</span>
<span class="sd">            preserves the magnitude of the variance of the weights in the</span>
<span class="sd">            forward pass. Choosing `&#39;fan_out&#39;` preserves the magnitudes in the</span>
<span class="sd">            backwards pass.</span>
<span class="sd">        nonlinearity: str, default: &#39;leaky_relu&#39;</span>
<span class="sd">            The non-linear function method name,</span>
<span class="sd">            recommended to use only with `&#39;relu&#39;` or `&#39;leaky_relu&#39;`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).kaiming_normal(mode=&#39;fan_in&#39;, nonlinearity=&#39;relu&#39;)</span>

<span class="sd">        .. _Delving deep into rectifiers: https://arxiv.org/abs/1502.01852</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>    </div>

<div class="viewcode-block" id="Tensor.sparse"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sparse.html#giagrad.Tensor.sparse">[docs]</a>    <span class="k">def</span> <span class="nf">sparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills the 2D Tensor data as a sparse matrix.</span>

<span class="sd">        Non-zero elements will be drawn from the normal distribution</span>
<span class="sd">        :math:`\mathcal{N}(0, \text{sigma})`, as described in `Deep learning via</span>
<span class="sd">        Hessian-free optimization` - Martens, J. (2010).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        sparsity: float between [0, 1) </span>
<span class="sd">            The fraction of elements in each column to be set to zero.</span>
<span class="sd">        std: float </span>
<span class="sd">            The standard deviation of the normal distribution used to generate</span>
<span class="sd">            the non-zero values.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).sparse(sparsity=0.4, std=0.2)</span>

<span class="sd">        .. _Deep learning via Hessian-free optimization: https://dl.acm.org/doi/10.5555/3104322.3104416</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init</span><span class="o">.</span><span class="n">sparse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparsity</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="Tensor.orthogonal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.orthogonal.html#giagrad.Tensor.orthogonal">[docs]</a>    <span class="k">def</span> <span class="nf">orthogonal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fills Tensor data with a (semi) orthogonal matrix.</span>

<span class="sd">        Values are generated according to the method described in </span>
<span class="sd">        Exact solutions to the nonlinear dynamics of learning in deep</span>
<span class="sd">        linear neural networks - Saxe, A. et al. (2013). The Tensor must have</span>
<span class="sd">        at least 2 dimensions, and for Tensors with more than 2 dimensions the</span>
<span class="sd">        trailing dimensions are flattened.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        gain: float</span>
<span class="sd">            Optional scaling factor.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor.empty(3, 5).orthogonal()</span>
<span class="sd">        &quot;&quot;&quot;</span> 
        <span class="n">init</span><span class="o">.</span><span class="n">orthogonal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="c1">### MATH ###</span>
<div class="viewcode-block" id="Tensor.comm"><a class="viewcode-back" href="../../generated/giagrad.Tensor.comm.html#giagrad.Tensor.comm">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">comm</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">operator</span><span class="p">:</span> <span class="n">Context</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new instance of an autodifferentiable tensor given a :class:`giagrad.tensor.Context` operator.</span>

<span class="sd">        ``comm`` creates a tensor with the output of :func:`~giagrad.tensor.Context.forward`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *tensors: array_like, ... </span>
<span class="sd">            Internally ``comm`` transforms any object in ``*tensors`` to a Tensor and </span>
<span class="sd">            passes it to :func:`~giagrad.tensor.Context.forward`.</span>

<span class="sd">        *kwargs: </span>
<span class="sd">            Optional arguments passed to the :func:`~giagrad.tensor.Context.forward` method </span>
<span class="sd">            of the ``operator`` parameter.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from giagrad.mlops import Softmax</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.27639335  0.7524293   0.69203097]</span>
<span class="sd">                 [ 0.37772807 -0.9291505  -0.80418533]]</span>
<span class="sd">        &gt;&gt;&gt; Tensor.comm(Softmax, t, axis=1)</span>
<span class="sd">        tensor: [[0.24242324 0.390224   0.36735278]</span>
<span class="sd">                 [0.6339727  0.17159334 0.19443396]] grad_fn: Softmax(axis = 1)</span>

<span class="sd">        .. _numpy.array: https://numpy.org/doc/stable/reference/generated/numpy.array.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">operands</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">]</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">context</span> <span class="o">=</span> <span class="n">operator</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">operands</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span></div>
    

    <span class="c1"># ***** math functions (unary) ***** </span>
<div class="viewcode-block" id="Tensor.sqrt"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sqrt.html#giagrad.Tensor.sqrt">[docs]</a>    <span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the square-root of the elements of `data`. See :func:`~giagrad.Tensor.pow`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.square"><a class="viewcode-back" href="../../generated/giagrad.Tensor.square.html#giagrad.Tensor.square">[docs]</a>    <span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the square of the elements of `data`. See :func:`~giagrad.Tensor.pow`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.exp"><a class="viewcode-back" href="../../generated/giagrad.Tensor.exp.html#giagrad.Tensor.exp">[docs]</a>    <span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the exponential of the elements of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \exp^{data_i}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor([0, 0.6931471805599453]).exp()</span>
<span class="sd">        tensor: [1. 2.] grad_fn: Exp</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Exp</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.log"><a class="viewcode-back" href="../../generated/giagrad.Tensor.log.html#giagrad.Tensor.log">[docs]</a>    <span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the natural logarithm of the elements of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \log_e(data_i)</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(3).uniform() * 1e4</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [9553.524  3221.3936 6511.507 ] grad_fn: Mul</span>
<span class="sd">        &gt;&gt;&gt; t.log()</span>
<span class="sd">        tensor: [7.650997 8.125444 8.514212] grad_fn: Ln</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Log</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.reciprocal"><a class="viewcode-back" href="../../generated/giagrad.Tensor.reciprocal.html#giagrad.Tensor.reciprocal">[docs]</a>    <span class="k">def</span> <span class="nf">reciprocal</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the reciprocal of the elements of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{data_i}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(3).uniform() </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [0.00142364 0.8617358  0.30606526]</span>
<span class="sd">        &gt;&gt;&gt; t.reciprocal()</span>
<span class="sd">        tensor: [702.4239      1.1604484   3.267277 ] grad_fn: Reciprocal</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Reciprocal</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.abs"><a class="viewcode-back" href="../../generated/giagrad.Tensor.abs.html#giagrad.Tensor.abs">[docs]</a>    <span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the absolute value of the elements of `data`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \lvert data_i \rvert</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; Tensor([-1, -2, -3]).abs()</span>
<span class="sd">        tensor: [1. 2. 3.] grad_fn: Abs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Abs</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> </div>

    <span class="c1"># ***** math functions (binary) *****</span>
<div class="viewcode-block" id="Tensor.add"><a class="viewcode-back" href="../../generated/giagrad.Tensor.add.html#giagrad.Tensor.add">[docs]</a>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the sum of `data` and ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object to add to `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__add__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.sub"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sub.html#giagrad.Tensor.sub">[docs]</a>    <span class="k">def</span> <span class="nf">sub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the substraction of ``other`` from `data`.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object to substract from `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__sub__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.mul"><a class="viewcode-back" href="../../generated/giagrad.Tensor.mul.html#giagrad.Tensor.mul">[docs]</a>    <span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the multiplication of `data` to ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object that multiplies `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__mul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.pow"><a class="viewcode-back" href="../../generated/giagrad.Tensor.pow.html#giagrad.Tensor.pow">[docs]</a>    <span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with `data` raised to the power of ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object that `data` is raised to.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__pow__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.matmul"><a class="viewcode-back" href="../../generated/giagrad.Tensor.matmul.html#giagrad.Tensor.matmul">[docs]</a>    <span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the matrix multiplication of `data` and ``other``.</span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like </span>
<span class="sd">            The array_like object that `data` is multiplied to from the left-hand side.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__matmul__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.div"><a class="viewcode-back" href="../../generated/giagrad.Tensor.div.html#giagrad.Tensor.div">[docs]</a>    <span class="k">def</span> <span class="nf">div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new tensor with the division of `data` to ``other``.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        other: array_like or float</span>
<span class="sd">            The number or object that divides `data`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__truediv__</span><span class="p">(</span><span class="n">other</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="mf">0.0</span><span class="o">-</span><span class="bp">self</span> <span class="c1"># Tensor.comm(mops.Mul, self, -1)</span>
    <span class="c1"># TODO</span>
    <span class="k">def</span> <span class="nf">clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">):</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span> <span class="c1"># ((self-min_).relu()+min_) - (self-max_).relu()</span>
    <span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span> <span class="c1"># return self / (self.abs() + 1e-10)</span>

    <span class="c1"># ***** activation functions (unary) ***** </span>
<div class="viewcode-block" id="Tensor.relu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.relu.html#giagrad.Tensor.relu">[docs]</a>    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the Rectified Linear Unit (ReLU) function element-wise. See `ReLU`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \max(0, data)</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.96863234  0.64852756 -0.52318954]</span>
<span class="sd">                 [-0.18809071 -0.48402452  0.86754996]]</span>
<span class="sd">        &gt;&gt;&gt; t.relu()</span>
<span class="sd">        tensor: [[0.96863234 0.64852756 0.        ]</span>
<span class="sd">                 [0.         0.         0.86754996]] grad_fn: ReLU</span>

<span class="sd">        .. _ReLU: https://paperswithcode.com/method/relu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.sigmoid"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sigmoid.html#giagrad.Tensor.sigmoid">[docs]</a>    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise sigmoid function. See `sigmoid`_.</span>

<span class="sd">        For numerical stability sigmoid function is computed with `numpy.logaddexp`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{(1 + \exp(-data_i))}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-100, 100)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-49.970577  35.522175 -14.944364]</span>
<span class="sd">                 [ 32.187164 -66.65264   48.01228 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.sigmoid()</span>
<span class="sd">        tensor: [[1.9863422e-22 1.0000000e+00 3.2340398e-07]</span>
<span class="sd">                 [1.0000000e+00 1.1301229e-29 1.0000000e+00]] grad_fn: Sigmoid</span>

<span class="sd">        .. _numpy.logaddexp: https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html</span>
<span class="sd">        .. _sigmoid: https://paperswithcode.com/method/sigmoid-activation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.elu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.elu.html#giagrad.Tensor.elu">[docs]</a>    <span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Exponential Linear Unit (ELU) function to `data`. See `ELU`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i =</span>
<span class="sd">            \begin{cases} </span>
<span class="sd">                data_i \ \ if \ \ data_i &gt; 0 \\ </span>
<span class="sd">                \text{alpha}(\exp(data_i) - 1) \ \ if \ \ x \leq 0 \\</span>
<span class="sd">            \end{cases}</span>
<span class="sd">            </span>
<span class="sd">        .. _ELU: https://paperswithcode.com/method/elu</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        alpha: float</span>
<span class="sd">            The :math:`\alpha` value for the ELU formulation.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-100, 100)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-49.970577  35.522175 -14.944364]</span>
<span class="sd">                 [ 32.187164 -66.65264   48.01228 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.elu()</span>
<span class="sd">        tensor: [[-1.        35.522175  -0.9999997]</span>
<span class="sd">                 [32.187164  -1.        48.01228  ]] grad_fn: ELU(alpha=1.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">ELU</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.silu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.silu.html#giagrad.Tensor.silu">[docs]</a>    <span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise Sigmoid-Weighted Linear Unit (SiLU) function,</span>
<span class="sd">        also called Swish. See `Swish`_.</span>
<span class="sd">    </span>
<span class="sd">        For numerical stability SiLU is computed with `numpy.logaddexp`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{data_i}{(1 + \exp(\text{beta} \cdot -data_i))} </span>
<span class="sd">        </span>
<span class="sd">        .. _Swish: https://paperswithcode.com/method/swish</span>
<span class="sd">        .. _numpy.logaddexp: https://numpy.org/doc/stable/reference/generated/numpy.logaddexp.html</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        beta: float</span>
<span class="sd">            Hyperparameter for Swish formulation.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-10, 10)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 5.4958744   0.13549101 -4.5210676 ]</span>
<span class="sd">                 [-1.7155124   5.2369795  -7.6546626 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.silu()</span>
<span class="sd">        tensor: [[ 5.4734135e+00  7.2327957e-02 -4.8648320e-02]</span>
<span class="sd">                 [-2.6153007e-01  5.2092857e+00 -3.6252895e-03]] grad_fn: SiLU(beta=1.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">SiLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.tanh"><a class="viewcode-back" href="../../generated/giagrad.Tensor.tanh.html#giagrad.Tensor.tanh">[docs]</a>    <span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the Tanh function element-wise. See `Tanh`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{e^{data_i} - e^{-data_i}}{e^{data_i} + e^{-data_i}}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-8, 8)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-0.42122853 -3.4285958   7.846644  ]</span>
<span class="sd">                 [ 0.7483299   6.6553855   3.3439522 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.tanh()                                                                                             </span>
<span class="sd">        tensor: [[-0.3979649  -0.9978985   0.9999997 ]</span>
<span class="sd">                 [ 0.6341515   0.99999666  0.9975113 ]] grad_fn: tanh</span>

<span class="sd">        .. _Tanh: https://paperswithcode.com/method/tanh-activation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.leakyrelu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.leakyrelu.html#giagrad.Tensor.leakyrelu">[docs]</a>    <span class="k">def</span> <span class="nf">leakyrelu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Leaky Rectified Linear Unit (Leaky ReLU) function to `data`. </span>
<span class="sd">        See `Leaky ReLU`_ .</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i =</span>
<span class="sd">            \begin{cases} </span>
<span class="sd">                data_i \ \ if \ \ data_i &gt; 0 \\ </span>
<span class="sd">                \text{neg_slope} \cdot data_i \ \ if \ \ x \leq 0 \\</span>
<span class="sd">            \end{cases}</span>
<span class="sd">        </span>
<span class="sd">        .. _Leaky ReLU: https://paperswithcode.com/method/elu</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        neg_slope: float</span>
<span class="sd">            Controls de angle of the negative slope (which only affects negative input values).</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, requires_grad=True).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-0.83589154  0.8874637  -0.465633  ]</span>
<span class="sd">                 [-0.5879877   0.22095676 -0.0592072 ]]</span>
<span class="sd">        &gt;&gt;&gt; d = t.leakyrelu(neg_slope=3)                                                                         </span>
<span class="sd">        &gt;&gt;&gt; d</span>
<span class="sd">        tensor: [[-2.5076747   0.8874637  -1.396899  ]</span>
<span class="sd">                 [-1.7639632   0.22095676 -0.17762159]] grad_fn: LeakyReLU(neg_slope=3)</span>
<span class="sd">        &gt;&gt;&gt; d.backward()                                                                                         </span>
<span class="sd">        &gt;&gt;&gt; t.grad</span>
<span class="sd">        array([[3., 1., 3.],</span>
<span class="sd">               [3., 1., 3.]], dtype=float32)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">neg_slope</span><span class="o">=</span><span class="n">neg_slope</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.softplus"><a class="viewcode-back" href="../../generated/giagrad.Tensor.softplus.html#giagrad.Tensor.softplus">[docs]</a>    <span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mf">20.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the Softplus function element-wise. See `Softplus`_.</span>

<span class="sd">        For numerical stability the implementation reverts to the linear function when</span>
<span class="sd">        :math:`data_i \times \text{beta} &gt; \text{limit}`.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \frac{1}{\text{beta}} \cdot \log(1 + \exp(\text{beta} \cdot data_i))</span>
<span class="sd">    </span>
<span class="sd">        .. _Softplus: https://paperswithcode.com/method/softplus</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        beta: float</span>
<span class="sd">            The :math:`\beta` value for the Softplus formulation.</span>
<span class="sd">        limit: float</span>
<span class="sd">            Data times beta above this revert to a linear function.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t                                                                                                    </span>
<span class="sd">        tensor: [[ 0.54631704 -0.703394    0.85786563]</span>
<span class="sd">                 [-0.24458279  0.23733494 -0.32190484]]</span>
<span class="sd">        &gt;&gt;&gt; t.softplus(beta=5, limit=1)</span>
<span class="sd">        tensor: [[0.54631704 0.00585142 0.85786563]</span>
<span class="sd">                 [0.05160499 0.23733494 0.03646144]] grad_fn: Softplus(lim=1, alpha=5)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Softplus</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.quick_gelu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.quick_gelu.html#giagrad.Tensor.quick_gelu">[docs]</a>    <span class="k">def</span> <span class="nf">quick_gelu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise Quick GELU. See `GELU`_.</span>
<span class="sd">        </span>
<span class="sd">        Quick GELU is an approximation of GELU through :func:`~giagrad.Tensor.silu` </span>
<span class="sd">        with alpha = 1.702 to ease GELU&#39;s computational complexity. </span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)                                                       </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.62271285  0.37412217 -0.6465454 ]</span>
<span class="sd">                 [-0.9013401  -0.02915052 -0.9814293 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.quick_gelu()</span>
<span class="sd">        tensor: [[ 0.4624659   0.2446833  -0.16141725]</span>
<span class="sd">                 [-0.15989538 -0.01421376 -0.15543076]] grad_fn: QuickGELU</span>

<span class="sd">        .. _GELU: https://paperswithcode.com/method/gelu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">SiLU</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.702</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.gelu"><a class="viewcode-back" href="../../generated/giagrad.Tensor.gelu.html#giagrad.Tensor.gelu">[docs]</a>    <span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Gaussina Error Linear Unit (Leaky ReLU) function to `data`. </span>
<span class="sd">        See `GELU`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i = data_i \ \Phi(data_i) </span>
<span class="sd">                = data_i \cdot \frac{1}{2} \left[1 + \text{erf}(\frac{data_i}{\sqrt{2}})\right]</span>
<span class="sd">        </span>
<span class="sd">        Where :math:`\Phi` is the Gaussian cumulative distribution function.</span>
<span class="sd">    </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t                                                                                                    </span>
<span class="sd">        tensor: [[-0.42565832  0.8579072  -0.40772486]</span>
<span class="sd">                 [ 0.4038496   0.09953032 -0.6694602 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.gelu()                                                                                             </span>
<span class="sd">        tensor: [[-0.14268097  0.6901076  -0.1393431 ]</span>
<span class="sd">                 [ 0.26525608  0.05371065 -0.1684846 ]] grad_fn: GELU</span>

<span class="sd">        .. _GELU: https://paperswithcode.com/method/gelu</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.relu6"><a class="viewcode-back" href="../../generated/giagrad.Tensor.relu6.html#giagrad.Tensor.relu6">[docs]</a>    <span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies a modified version of ReLU with maximum size of 6. See `ReLU6`_.</span>

<span class="sd">        .. math::</span>
<span class="sd">            out_i = \min(\max(0, x), 6)</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 20)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[11.792983   -0.20050316 15.441884  ]</span>
<span class="sd">                 [ 3.5337465  13.230399    9.813518  ]]</span>
<span class="sd">        &gt;&gt;&gt; t.relu6()</span>
<span class="sd">        tensor: [[6.        0.        6.       ]</span>
<span class="sd">                 [3.5337465 6.        6.       ]] grad_fn: ReLU6</span>

<span class="sd">        .. _ReLU6: https://paperswithcode.com/method/relu6</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.mish"><a class="viewcode-back" href="../../generated/giagrad.Tensor.mish.html#giagrad.Tensor.mish">[docs]</a>    <span class="k">def</span> <span class="nf">mish</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mf">20.</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a new Tensor with element-wise Mish function. See `Mish`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i = data_i \cdot \text{tanh} \, \text{softplus}(data_i)</span>
<span class="sd">        </span>
<span class="sd">        See :func:`~giagrad.Tensor.softplus`</span>

<span class="sd">        .. _Mish: https://paperswithcode.com/method/mish</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        beta: float</span>
<span class="sd">            The :math:`\beta` value for the Softplus formulation.</span>
<span class="sd">        limit: float</span>
<span class="sd">            Data times beta above limit reverts to a linear function in </span>
<span class="sd">            Softplus formulation.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-5, 5)                                                                </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-1.3851491  1.2130666 -4.9049625]</span>
<span class="sd">                 [ 2.6859815 -4.845946   2.1385565]]</span>
<span class="sd">        &gt;&gt;&gt; t.mish()                                                                                             </span>
<span class="sd">        tensor: [[-0.3043592   1.0920179  -0.03620962]</span>
<span class="sd">                 [ 2.6642     -0.03794033  2.0915585 ]] grad_fn: Mish(beta=1.0, lim=20.0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Mish</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">)</span> </div>

<div class="viewcode-block" id="Tensor.hardswish"><a class="viewcode-back" href="../../generated/giagrad.Tensor.hardswish.html#giagrad.Tensor.hardswish">[docs]</a>    <span class="k">def</span> <span class="nf">hardswish</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a new Tensor applying Hard Swish function to `data`. See `Hard Swish`_.</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            out_i = data_i \, \frac{\text{ReLU6}(data_i + 3)}{6}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 4).uniform(-5, 5)                                                                 </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-4.0175104   3.993501   -1.0318986  -0.30065283]</span>
<span class="sd">                 [-2.4765007  -1.3878915   1.7888396   4.3194094 ]]</span>
<span class="sd">        &gt;&gt;&gt; t.hardswish()                                                                 </span>
<span class="sd">        tensor: [[-0.          3.993501   -0.3384802  -0.13526106]</span>
<span class="sd">                 [-0.21607438 -0.37290528  1.4277442   4.3194094 ]] grad_fn: Hardswish</span>
<span class="sd">        </span>
<span class="sd">        .. _Hard Swish: https://paperswithcode.com/method/hard-swish</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Hardswish</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.softmax"><a class="viewcode-back" href="../../generated/giagrad.Tensor.softmax.html#giagrad.Tensor.softmax">[docs]</a>    <span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies Softmax function to every 1-D slice defined by ``axis``. See `Softmax`_.</span>

<span class="sd">        The elements of the n-dimensinal output Tensor will lie in de range :math:`[0, 1]`</span>
<span class="sd">        and sum to :math:`1`.</span>
<span class="sd">        </span>
<span class="sd">        Softmax for a one-dimensional slice is defined as:</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            \text{Softmax}(x_i) = \frac{\exp(x_i)}{\sum_j \exp(x_j)} </span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: int</span>
<span class="sd">            The dimension along which Softmax will be computed (so every slice along dim </span>
<span class="sd">            will sum to 1).</span>


<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[ 0.27639335  0.7524293   0.69203097]</span>
<span class="sd">                 [ 0.37772807 -0.9291505  -0.80418533]]</span>
<span class="sd">        &gt;&gt;&gt; t.softmax(axis=1)</span>
<span class="sd">        tensor: [[0.24242324 0.390224   0.36735278]</span>
<span class="sd">                 [0.6339727  0.17159334 0.19443396]] grad_fn: Softmax(axis = 1)</span>

<span class="sd">        .. _Softmax: https://paperswithcode.com/method/softmax</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">Softmax</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.log_softmax"><a class="viewcode-back" href="../../generated/giagrad.Tensor.log_softmax.html#giagrad.Tensor.log_softmax">[docs]</a>    <span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies LogSoftmax function to every 1-D slice defined by ``axis``.</span>

<span class="sd">        LogSoftmax for a one-dimensional slice is defined as:</span>
<span class="sd">        </span>
<span class="sd">        .. math::</span>
<span class="sd">            \text{LogSoftmax}(x_i) = \log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)} \right)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: int</span>
<span class="sd">            The dimension along which LogSoftmax will be computed.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3).uniform(-1, 1)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[-0.07469178  0.7226724   0.98966014]</span>
<span class="sd">                 [-0.01990889 -0.4521888   0.26520386]]</span>
<span class="sd">        &gt;&gt;&gt; t.softmax(axis=1)</span>
<span class="sd">        tensor: [[-0.72091377 -0.26915795 -0.39513725]</span>
<span class="sd">                 [-0.6661309  -1.4440191  -1.1195936 ]] grad_fn: LogSoftmax(axis = 0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mlops</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>

    <span class="c1"># ***** math functions (binary) *****</span>
    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Add</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Add</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Sub</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Sub</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Mul</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Mul</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Pow</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Pow</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Matmul</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__rmatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Matmul</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Div</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> 
    <span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">mops</span><span class="o">.</span><span class="n">Div</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span> 

    <span class="c1"># ***** math functions autossign (i.e. a += b) *****</span>
    <span class="k">def</span> <span class="fm">__iadd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__isub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__imul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">*=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">**=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__itruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">/=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>
    <span class="k">def</span> <span class="fm">__imatmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">@</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">;</span> <span class="k">return</span> <span class="bp">self</span>

    <span class="c1"># ***** math functions (reduction) *****</span>
<div class="viewcode-block" id="Tensor.mean"><a class="viewcode-back" href="../../generated/giagrad.Tensor.mean.html#giagrad.Tensor.mean">[docs]</a>    <span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the mean value of each 1-D slice of the tensor in the given ``axis``, if ``axis`` is </span>
<span class="sd">        a list of dimensions, reduce over all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as input except in the ``axis`` </span>
<span class="sd">        where it is of size 1. Otherwise, every ``axis`` is squeezed, leading to an output tensor</span>
<span class="sd">        with fewer dimensions. If no ``axis`` is supplied all data is reduced to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, mean reduces all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor(np.arange(12).reshape((2,2,3)))</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[ 0.  1.  2.]</span>
<span class="sd">                  [ 3.  4.  5.]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 6.  7.  8.]</span>
<span class="sd">                  [ 9. 10. 11.]]]</span>
<span class="sd">        &gt;&gt;&gt; t.mean(axis=(0, 1), keepdims=True)                                      </span>
<span class="sd">        tensor: [[[4.5 5.5 6.5]]] grad_fn: Mean(axis = (0, 1))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">Mean</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.sum"><a class="viewcode-back" href="../../generated/giagrad.Tensor.sum.html#giagrad.Tensor.sum">[docs]</a>    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the sum of each 1-D slice of the tensor in the given ``axis``, if ``axis`` is </span>
<span class="sd">        a list of dimensions, reduce over all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as input except in the ``axis`` </span>
<span class="sd">        where it is of size 1. Otherwise, every ``axis`` is squeezed, leading to an output tensor</span>
<span class="sd">        with fewer dimensions. If no ``axis`` is supplied all data is reduced to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, sum reduces all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, 4, dtype=int).uniform(0, 5)                          </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[2 0 0 3]</span>
<span class="sd">                  [0 2 1 4]</span>
<span class="sd">                  [4 0 0 2]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[3 1 4 0]</span>
<span class="sd">                  [3 3 4 3]</span>
<span class="sd">                  [4 0 1 0]]]</span>
<span class="sd">        &gt;&gt;&gt; t.sum(axis=2, keepdims=True)                           </span>
<span class="sd">        tensor: [[[ 5.]</span>
<span class="sd">                  [ 7.]</span>
<span class="sd">                  [ 6.]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 8.]</span>
<span class="sd">                  [13.]</span>
<span class="sd">                  [ 5.]]] grad_fn: Sum(axis = 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">Sum</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span></div>
    
<div class="viewcode-block" id="Tensor.max"><a class="viewcode-back" href="../../generated/giagrad.Tensor.max.html#giagrad.Tensor.max">[docs]</a>    <span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the maximum value of each 1-D slice of the tensor in the given ``axis``, if ``axis`` is </span>
<span class="sd">        a list of dimensions, reduce over all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as input except in the ``axis`` </span>
<span class="sd">        where it is of size 1. Otherwise, every ``axis`` is squeezed, leading to an output tensor</span>
<span class="sd">        with fewer dimensions. If no ``axis`` is supplied all data is reduced to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, max reduces all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, 4, dtype=np.int8).uniform(0, 100)</span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[54 83 83 67]</span>
<span class="sd">                  [81 64 76 51]</span>
<span class="sd">                  [76 98 58 28]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[64 91 59 48]</span>
<span class="sd">                  [70 41 16 33]</span>
<span class="sd">                  [27 44 17 70]]]</span>
<span class="sd">        &gt;&gt;&gt; t.max(axis=(1, 2))                                                          </span>
<span class="sd">        tensor: [98. 91.] grad_fn: Max(axis = (1, 2))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.min"><a class="viewcode-back" href="../../generated/giagrad.Tensor.min.html#giagrad.Tensor.min">[docs]</a>    <span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the minimum value of each 1-D slice of the tensor in the given ``axis``, if ``axis`` is </span>
<span class="sd">        a list of dimensions, reduce over all of them.</span>

<span class="sd">        If keepdims is True, the output tensor is of the same size as input except in the ``axis`` </span>
<span class="sd">        where it is of size 1. Otherwise, every ``axis`` is squeezed, leading to an output tensor</span>
<span class="sd">        with fewer dimensions. If no ``axis`` is supplied all data is reduced to a scalar value.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        axis: (int, ...) or None, default: None</span>
<span class="sd">            The dimension or dimension to reduce. If None, min reduces all dimensions.</span>
<span class="sd">        keepdims: bool, default: False</span>
<span class="sd">            Whether te output tensor should retain the reduced dimensions.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; t = Tensor.empty(2, 3, 4, dtype=np.int8).uniform(0, 20)                     </span>
<span class="sd">        &gt;&gt;&gt; t</span>
<span class="sd">        tensor: [[[ 3 14 15  7]</span>
<span class="sd">                  [18  9 11 18]</span>
<span class="sd">                  [16 17 14  9]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[ 5  3 12 18]</span>
<span class="sd">                  [15 11 15  1]</span>
<span class="sd">                  [13  2  2 10]]]</span>
<span class="sd">        &gt;&gt;&gt; t.min(axis=2, keepdims=True)                                                </span>
<span class="sd">        tensor: [[[3.]</span>
<span class="sd">                  [9.]</span>
<span class="sd">                  [9.]]</span>
<span class="sd">        ...</span>
<span class="sd">                 [[3.]</span>
<span class="sd">                  [1.]</span>
<span class="sd">                  [2.]]] grad_fn: Min(axis = 2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">rops</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">)</span></div>

    <span class="c1"># ***** shape functions (reduction) *****</span>
    <span class="c1"># this operators create views</span>
<div class="viewcode-block" id="Tensor.permute"><a class="viewcode-back" href="../../generated/giagrad.Tensor.permute.html#giagrad.Tensor.permute">[docs]</a>    <span class="k">def</span> <span class="nf">permute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a view with ``axis`` permuted.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Permute</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>
<div class="viewcode-block" id="Tensor.transpose"><a class="viewcode-back" href="../../generated/giagrad.Tensor.transpose.html#giagrad.Tensor.transpose">[docs]</a>    <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Permutes two specific dimensions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Permute</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim0</span><span class="p">))</span></div>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="sd">&quot;&quot;&quot;Returns a transposed view of a 2 dimensional Tensor.&quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Dimensions = 2 required, this is matrix transposition&quot;</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm</span><span class="p">(</span><span class="n">sops</span><span class="o">.</span><span class="n">Permute</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner"></div>
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
       Copyright 2023, Carlos Prez.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://sphinx-doc.org/">Sphinx</a> 6.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>