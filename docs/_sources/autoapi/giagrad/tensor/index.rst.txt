:py:mod:`giagrad.tensor`
========================

.. py:module:: giagrad.tensor


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   giagrad.tensor.Context
   giagrad.tensor.Tensor




.. py:class:: Context(save_for_backward: Tuple[Tensor, Ellipsis])

   Bases: :py:obj:`abc.ABC`

   Abstract class for all operators defined in mathops, reductionsops, etc
   An operator creates an instance of itself with class method forward that
   contains the parents of the Tensor created by Tensor.comm()

   Operators are just extensions of Tensor class to have Tensor functionality 
   self contained but separated in different files. 

   Attributes
   ----------
   parents: Tuple[Any, ...]
       operands/Tensors of the operator, can contain other values with Tensor.comm(.., **kwargs)


   .. py:method:: forward(*tensors, **kwargs) -> Tuple[Union[numpy.typing.NDArray, float], Context]
      :classmethod:
      :abstractmethod:

      Main function for forward pass.


   .. py:method:: backward(partial: numpy.typing.NDArray)
      :abstractmethod:

      Backprop automatic differentiation, to update grad of parents.
      partial: gradient of the output of forward method.


   .. py:method:: __str__()
      :abstractmethod:

      For graphviz visualization.



.. py:class:: Tensor(data, requires_grad: bool = False, context: Optional[Context] = None, name: str = '', dtype=np.float32)

   .. py:property:: shape
      :type: Tuple[int, Ellipsis]


   .. py:property:: dtype
      :type: type


   .. py:property:: size
      :type: int


   .. py:property:: ndim
      :type: int


   .. py:property:: T


   .. py:attribute:: __array_ufunc__

      Attributes
      ----------
      data: np.ndarray
          weights
      grad: NDArray
          gradients
      requires_grad: bool
          indicates if automatic differentiation is needed
      name: Optional[str]
          variable name, for visualization
      _ctx: Context
          class Context: defines parent nodes and function that created it
              like _prev and _op in micrograd.


   .. py:attribute:: __slots__
      :value: ['data', 'grad', '_ctx', 'requires_grad', 'name']

      

   .. py:method:: backward(debug: bool = False)

      https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py
      a.k.a topological sort / postorder then reversed


   .. py:method:: no_grad() -> Tensor


   .. py:method:: requires_grad_() -> Tensor


   .. py:method:: __repr__()

      Return repr(self).


   .. py:method:: __str__()

      Return str(self).


   .. py:method:: empty(*shape, **kwargs) -> Tensor
      :classmethod:


   .. py:method:: zeros()


   .. py:method:: ones()


   .. py:method:: constant(val)


   .. py:method:: normal(mu, sigma)


   .. py:method:: uniform(a, b)


   .. py:method:: dirac(groups=1)


   .. py:method:: xavier_uniform(gain=1)


   .. py:method:: xavier_normal(gain=1)


   .. py:method:: kaiming_uniform(neg_slope=0, mode='fan_in', nonlinearity='leaky_relu')


   .. py:method:: kaiming_normal(neg_slope=0, mode='fan_in', nonlinearity='leaky_relu')


   .. py:method:: sparse(sparsity, std=0.01)


   .. py:method:: orthogonal(gain=1)


   .. py:method:: comm(operator: Context, *tensors, **kwargs) -> Tensor
      :classmethod:


   .. py:method:: sqrt()


   .. py:method:: square()


   .. py:method:: exp()


   .. py:method:: log()


   .. py:method:: reciprocal()


   .. py:method:: abs()


   .. py:method:: __neg__()


   .. py:method:: clip(min_, max_)
      :abstractmethod:


   .. py:method:: sign()
      :abstractmethod:


   .. py:method:: relu()


   .. py:method:: sigmoid()


   .. py:method:: elu(alpha=1.0)


   .. py:method:: silu(beta=1.0)


   .. py:method:: tanh()


   .. py:method:: leakyrelu(neg_slope=0.01)


   .. py:method:: softplus(limit=20, beta=1)


   .. py:method:: quick_gelu()


   .. py:method:: gelu()


   .. py:method:: relu6()


   .. py:method:: mish()


   .. py:method:: hardswish()


   .. py:method:: softmax(axis: int)


   .. py:method:: log_softmax(axis: int)


   .. py:method:: __add__(x)


   .. py:method:: __radd__(x)


   .. py:method:: __sub__(x)


   .. py:method:: __rsub__(x)


   .. py:method:: __mul__(x)


   .. py:method:: __rmul__(x)


   .. py:method:: __pow__(x)


   .. py:method:: __rpow__(x)


   .. py:method:: __matmul__(x)


   .. py:method:: __rmatmul__(x)


   .. py:method:: __truediv__(x)


   .. py:method:: __rtruediv__(x)


   .. py:method:: __iadd__(x)


   .. py:method:: __isub__(x)


   .. py:method:: __imul__(x)


   .. py:method:: __ipow__(x)


   .. py:method:: __itruediv__(x)


   .. py:method:: __imatmul__(x)


   .. py:method:: mean(axis: Union[Tuple[int, Ellipsis], int, None] = None, keepdims: bool = False)


   .. py:method:: sum(axis: Union[Tuple[int, Ellipsis], int, None] = None, keepdims: bool = False)


   .. py:method:: max(axis: Union[Tuple[int, Ellipsis], int, None] = None, keepdims: bool = False)


   .. py:method:: min(axis: Union[Tuple[int, Ellipsis], int, None] = None, keepdims: bool = False)


   .. py:method:: add(x)


   .. py:method:: sub(x)


   .. py:method:: mul(x)


   .. py:method:: pow(x)


   .. py:method:: matmul(x)


   .. py:method:: div(x)


   .. py:method:: permute(axes=None)


   .. py:method:: transpose(dim0, dim1)



