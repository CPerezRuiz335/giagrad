{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b62753ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7caddfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ff21f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c44dd",
   "metadata": {},
   "source": [
    "# Binary operators\n",
    "## Dot product\n",
    "\n",
    "Parece que todo es \n",
    "\n",
    "```python\n",
    "a_grad = c_grad @ b.T \n",
    "b_grad = a.T @ c_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9dd732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backprop_torch(a, b, const=1):\n",
    "    a = torch.tensor(a, requires_grad=True)\n",
    "    b = torch.tensor(b, requires_grad=True)\n",
    "    c = a @ b\n",
    "    c.retain_grad()\n",
    "    (c.sum() * const).backward()\n",
    "    \n",
    "    print('a_grad_torch:\\n', a.grad.detach().numpy(), end='\\n'*2)\n",
    "    print('b_grad_torch:\\n', b.grad.detach().numpy(), end='\\n'*2)\n",
    "    print('c_grad_torch:\\n', c.grad.detach().numpy(), end='\\n'*2)\n",
    "\n",
    "def backprop(a, b, const=1):\n",
    "    c = a @ b\n",
    "    c_grad = np.ones_like(c)*const\n",
    "    \n",
    "    a_grad = c_grad @ b.T \n",
    "    b_grad = a.T @ c_grad\n",
    "    \n",
    "    print('a_grad:\\n', a_grad, end='\\n'*2)\n",
    "    print('b_grad:\\n', b_grad, end='\\n'*2)\n",
    "    print('c_grad:\\n', c_grad, end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7127e56",
   "metadata": {},
   "source": [
    "### Scalar matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae6f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad_torch:\n",
      " [[13.2]]\n",
      "\n",
      "b_grad_torch:\n",
      " [[6.6 6.6 6.6 6.6]]\n",
      "\n",
      "c_grad_torch:\n",
      " [[3.3 3.3 3.3 3.3]]\n",
      "\n",
      "a_grad:\n",
      " [[13.2]]\n",
      "\n",
      "b_grad:\n",
      " [[6.6 6.6 6.6 6.6]]\n",
      "\n",
      "c_grad:\n",
      " [[3.3 3.3 3.3 3.3]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2.]])\n",
    "b = np.array([[1.,1., 1., 1.]])\n",
    "\n",
    "backprop_torch(a, b, 3.3)\n",
    "backprop(a, b, 3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4cead",
   "metadata": {},
   "source": [
    "### Matrix vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9be9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad_torch:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "b_grad_torch:\n",
      " [[4.]\n",
      " [4.]]\n",
      "\n",
      "c_grad_torch:\n",
      " [[1.]\n",
      " [1.]]\n",
      "\n",
      "a_grad:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "b_grad:\n",
      " [[4.]\n",
      " [4.]]\n",
      "\n",
      "c_grad:\n",
      " [[1.]\n",
      " [1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2.,2.],\n",
    "              [2.,2.]])\n",
    "b = np.array([[1.,1.]]).T\n",
    "\n",
    "backprop_torch(a, b)\n",
    "backprop(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3821461",
   "metadata": {},
   "source": [
    "### Vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14826fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad_torch:\n",
      " [[4. 4.]]\n",
      "\n",
      "b_grad_torch:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "c_grad_torch:\n",
      " [[1. 1.]]\n",
      "\n",
      "a_grad:\n",
      " [[4. 4.]]\n",
      "\n",
      "b_grad:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "c_grad:\n",
      " [[1. 1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backprop_torch(b.T, a)\n",
    "backprop(b.T, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ecd9f",
   "metadata": {},
   "source": [
    "### Matrix Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bad190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad_torch:\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n",
      "\n",
      "b_grad_torch:\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n",
      "\n",
      "c_grad_torch:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "a_grad:\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n",
      "\n",
      "b_grad:\n",
      " [[4. 4.]\n",
      " [4. 4.]]\n",
      "\n",
      "c_grad:\n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backprop_torch(a, a)\n",
    "backprop(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf9439a",
   "metadata": {},
   "source": [
    "## Hadamard product\n",
    "\n",
    "Como multiplicacion de escalares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2713c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[4.,5.],\n",
    "              [1.,3.],\n",
    "              [9.,0.]])\n",
    "\n",
    "b = np.array([[2.,2.],\n",
    "              [2.,2.],\n",
    "              [2.,2.]])\n",
    "\n",
    "a_tensor = torch.tensor(a, requires_grad=True)\n",
    "b_tensor = torch.tensor(b, requires_grad=True)\n",
    "c_tensor = a_tensor * b_tensor\n",
    "c_tensor.retain_grad()\n",
    "c_tensor.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d37438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad_tensor:\n",
      " tensor([[6., 6.],\n",
      "        [6., 6.],\n",
      "        [6., 6.]], dtype=torch.float64)\n",
      "\n",
      "b_grad_tensor:\n",
      " tensor([[12., 15.],\n",
      "        [ 3.,  9.],\n",
      "        [27.,  0.]], dtype=torch.float64)\n",
      "\n",
      "c_grad_tensor:\n",
      " tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('a_grad_tensor:\\n', a_tensor.grad, end='\\n'*2)\n",
    "print('b_grad_tensor:\\n', b_tensor.grad, end='\\n'*2)\n",
    "print('c_grad_tensor:\\n', c_tensor.grad, end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f853c1",
   "metadata": {},
   "source": [
    "## Add\n",
    "\n",
    "Parece que vuelve a ser Hadamard product, cosa que makes sense.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial sum}{\\partial a} = \\frac{\\partial sum}{\\partial sum}  \n",
    "                                  \\frac{\\partial sum}{\\partial c} \n",
    "                                  \\frac{\\partial c}{\\partial a}\\\\\n",
    "\\frac{\\partial sum}{\\partial a} = 1\n",
    "                                  \\frac{\\partial sum}{\\partial c} \n",
    "                                  \\frac{\\partial c}{\\partial a}\\\\\n",
    "\\frac{\\partial c_{1,1}}{\\partial a_{1,1}} = a_{1,1} + b_{1,1} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70242afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[4.,5.],\n",
    "              [1.,3.],\n",
    "              [9.,0.]])\n",
    "\n",
    "b = np.array([[2.,2.],\n",
    "              [2.,2.],\n",
    "              [2.,2.]])\n",
    "\n",
    "a_tensor = torch.tensor(a, requires_grad=True)\n",
    "b_tensor = torch.tensor(b, requires_grad=True)\n",
    "c_tensor = a_tensor + b_tensor\n",
    "c_tensor.retain_grad()\n",
    "(c_tensor.sum()*3).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94dd9e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_grad_tensor:\n",
      " tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64)\n",
      "\n",
      "b_grad_tensor:\n",
      " tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64)\n",
      "\n",
      "c_grad_tensor:\n",
      " tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  7.],\n",
       "        [ 3.,  5.],\n",
       "        [11.,  2.]], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('a_grad_tensor:\\n', a_tensor.grad, end='\\n'*2)\n",
    "print('b_grad_tensor:\\n', b_tensor.grad, end='\\n'*2)\n",
    "print('c_grad_tensor:\\n', c_tensor.grad, end='\\n'*2)\n",
    "c_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047a93e",
   "metadata": {},
   "source": [
    "# Reduction operators\n",
    "\n",
    "## Sum\n",
    "\n",
    "Derivative of the sum of any Tensor is just zeros_like(tensor), but wat happens\n",
    "when it's combined with scalar multiplication on another Tensor of different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "65099955",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2.,2.],\n",
    "              [2.,2.],\n",
    "              [2.,2.]])\n",
    "\n",
    "b = np.array([[1.,2.],\n",
    "              [3.,4.],\n",
    "              [5.,6.]])\n",
    "\n",
    "a_tensor = torch.tensor(a, requires_grad=True)\n",
    "c_tensor = torch.tensor(b, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "af8ee092",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tensor = a_tensor.sum()\n",
    "b_tensor.retain_grad()\n",
    "\n",
    "d_tensor = (b_tensor * c_tensor).sum()\n",
    "d_tensor.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8ab119a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "39e79410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21., dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d24d39e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_tensor.sum() == b_tensor.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79e923",
   "metadata": {},
   "source": [
    "## Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "804702dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2.,2.],\n",
    "              [2.,2.],\n",
    "              [2.,2.]])\n",
    "\n",
    "b = np.array([[1.,2.],\n",
    "              [3.,4.],\n",
    "              [5.,6.]])\n",
    "\n",
    "a_tensor = torch.tensor(a, requires_grad=True)\n",
    "c_tensor = torch.tensor(b, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33b75dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tensor = a_tensor.max()\n",
    "b_tensor.retain_grad()\n",
    "\n",
    "d_tensor = (b_tensor * c_tensor).sum()\n",
    "d_tensor.retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e4b51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7d44e327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.],\n",
       "        [2., 2.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "901d9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor.max().sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9f0a624b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.1667],\n",
       "        [0.1667, 0.1667],\n",
       "        [0.1667, 0.1667]], dtype=torch.float64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
